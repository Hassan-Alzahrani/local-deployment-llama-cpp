{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97500427-9c68-4273-bcab-b0a64ff18a68",
   "metadata": {},
   "source": [
    "# Text Generation Basics\n",
    "\n",
    "This tutorial shows you how to control the text generation process and fine-tune the diversity, creativity, and quality of the generated text according to your needs. By adjusting these options and experimenting with different combinations of values, you can find the best settings for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d7fcc-b392-47b5-b6ab-7eccee6fc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"../models/gemma-1.1-7b-it.Q4_K_M.gguf\"\n",
    "PROMPT_FILE = \"../prompts/engaging-twitter-thread.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d0866-b4ff-43a8-94c2-9029c0abf4d4",
   "metadata": {},
   "source": [
    "## Random Number Generator (RNG) Seed\n",
    "\n",
    "The RNG seed is used to initialize the random number generator that influences the text generation process. By setting a specific value for `--seed` you can obtain consistent and reproducible results across multiple runs with the same input and settings. This can be helpful for testing, debugging, or comparing the effects of different options on the generated text to see when they diverge. If the seed is set to a value less than 0, a random seed will be used, which will result in different outputs on each run. The default value is -1 which will choose a random value for `--seed`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a676c8-5d90-425f-a900-b4b0694f38f8",
   "metadata": {},
   "source": [
    "### Random `--seed` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352f9d0-3861-4e6c-9b6c-1e3fb58ca8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --prompt \"Why is the sky blue?\" \\\n",
    "    --seed -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ecde3-e4ac-4983-a8cf-226bc70eef1d",
   "metadata": {},
   "source": [
    "### Fixed `--seed` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed26f2-7f4c-459b-93f6-782f8d4321b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --prompt \"Why is the sky blue?\" \\\n",
    "    --seed 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89e410-0143-4055-9639-4f3a2387930c",
   "metadata": {},
   "source": [
    "## Number of Tokens to Predict\n",
    "\n",
    "The `--predict N` (default: -1) controls the number of tokens the model generates in response to the input prompt. By adjusting this value, you can influence the length of the generated text. A higher value will result in longer text, while a lower value will produce shorter text.\n",
    "\n",
    "Even though all models have a finite context window, a value of -1 will enable *infinite* text generation. How? When the context window is full half of the tokens after `--keep N` will be discarded. The context must then be re-evaluated before generation can resume. On large models and/or large context windows, this can result in a significant pause in output. If the output delay is undesirable, a value of -2 will stop generation immediately when the context is filled.\n",
    "\n",
    "It is important to note that the generated text may be shorter than the specified number of tokens if an End-of-Sequence (EOS) token or a reverse prompt is encountered. In interactive mode, text generation will pause and control will be returned to the user. In non-interactive mode, the program will end. In both cases, the text generation may stop before reaching the specified `--predict` value. If you want the model to keep going without ever producing End-of-Sequence on its own, you can use the `--ignore-eos` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d133e58-15ff-4119-aaef-4b59256e26fb",
   "metadata": {},
   "source": [
    "### Default `--predict` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c43fe-6491-4060-a7ce-33db3364ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --prompt \"What is the meaning of life?\" \\\n",
    "    --predict -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12486c3-cdb9-47bb-b9a7-c3f65df82a64",
   "metadata": {},
   "source": [
    "### \"until context filled\" `--predict` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15de6a-4a69-4071-a46a-6de5f5f9f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --prompt \"What is the meaning of life?\" \\\n",
    "    --ctx-size 256 \\\n",
    "    --predict -2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569217e-1faa-4cd6-92dc-a2b9fdd77af5",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "Temperature, `--temp N` is a hyperparameter that controls the randomness of the generated text. It affects the probability distribution of the model's output tokens. A higher temperature makes the output more random and creative, while a lower temperature makes the output more focused, deterministic, and conservative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ec8d2-e59a-411b-9937-ea72d38cb8a0",
   "metadata": {},
   "source": [
    "### Default `--temp` example\n",
    "\n",
    "The default value is `--temp 0.8` which provides a balance between randomness and determinism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc1720-8fd5-4d4c-bd4d-46f0cced0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --seed 42 \\\n",
    "    --temp 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba68e0e-5a0b-4ce9-bbd2-5b3346498ecc",
   "metadata": {},
   "source": [
    "### Low `--temp` example\n",
    "\n",
    "At the extreme, a temperature of 0 will always pick the most likely next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aaa2b5-c8de-48d4-b52c-afa2d1f5493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --seed 42 \\\n",
    "    --temp 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b25125-44be-45d5-96c4-1816857fb59e",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Explore higher values for `--temp`. Is there a value of `--temp` which it \"too high\" for your model and prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e8ff5-a607-44aa-bd9c-8127540b8a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1023857f-bfbe-421f-8985-b525dbed9665",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f583ef51-1c95-4f29-b85b-aff440d87fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --seed 42 \\\n",
    "    --temp 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fca20a-29da-4883-847d-943cd29a4c78",
   "metadata": {},
   "source": [
    "## Repeat Penalty\n",
    "\n",
    "The `--repeat-penalty` option helps prevent the model from generating repetitive or monotonous text. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. The default value is 1 (which means no penalty).\n",
    "\n",
    "The `--repeat-last-n` option controls the number of tokens in the history to consider for penalizing repetition. A larger value will look further back in the generated text to prevent repetitions, while a smaller value will only consider recent tokens. A value of 0 disables the penalty, and a value of -1 sets the number of tokens considered equal to the context size, `--ctx-size`. The default value is 64. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227cd8c-8795-475b-acc0-871ba79f075b",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc3167-4d73-4136-94d6-685d0f753907",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --repeat-penalty 1.5 \\\n",
    "    --repeat-last-n 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd40af-3201-4d6b-96b2-a65e9a431689",
   "metadata": {},
   "source": [
    "## Top-K Sampling\n",
    "\n",
    "Top-k sampling is a text generation method that selects the next token only from the `--top-k` most likely tokens predicted by the model. It helps reduce the risk of generating low-probability or nonsensical tokens, but it may also limit the diversity of the output. A higher value for top-k (e.g., 100) will consider more tokens and lead to more diverse text, while a lower value (e.g., 10) will focus on the most probable tokens and generate more conservative text. The default value is 40.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7845a0e-f8a1-41d1-b265-6b99da840de0",
   "metadata": {},
   "source": [
    "### Default `--top-k` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ee26a-2fe2-42bb-9bcc-bfe143e6e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --top-k 40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba89dd3-9f88-4fd7-85e8-57d6139b8c9b",
   "metadata": {},
   "source": [
    "### Low `--top-k` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec410a-818d-4a2e-aea2-e84231d744b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --top-k 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66035e2-502f-4d74-982e-34d2357774c7",
   "metadata": {},
   "source": [
    "### High `--top-k` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97acf190-4394-4582-aae8-669bdb361bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --top-k 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592f98d-9a51-458b-b8c8-5171a85cddf6",
   "metadata": {},
   "source": [
    "## Top-P Sampling\n",
    "\n",
    "Top-p sampling, `top-p`, also known as nucleus sampling, is another text generation method that selects the next token from a subset of tokens that together have a cumulative probability of at least p. This method provides a balance between diversity and quality by considering both the probabilities of tokens and the number of tokens to sample from. A higher value for top-p (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. The default value is 0.9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4215bdb-cdef-4d9e-a875-f41c07c7131a",
   "metadata": {},
   "source": [
    "### Default `--top-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd5ac7-fef8-4449-a75c-dd6918826936",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --top-p 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b65992-578b-4b0e-a7c9-a023257ffca8",
   "metadata": {},
   "source": [
    "### Low `--top-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c1bfc-ac22-4fa8-8c57-6945016dafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --top-p 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88066c66-c556-4c51-b016-431fb4532fd8",
   "metadata": {},
   "source": [
    "### High `--top-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ba3c0-8646-4597-8d92-ea3dc7c7f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --top-p 0.99\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf85b2e-c110-4dd7-84c6-952048fb55c4",
   "metadata": {},
   "source": [
    "## Min-P Sampling\n",
    "\n",
    "The `--min-p` sampling method sets a minimum base probability threshold for token selection and aims to ensure a balance of quality and variety in the generated text. The `--min-p` method was designed as an alternative to `--top-p`. The parameter $p$ represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with $p=0.05$ and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. The default value is 0.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193778cd-271f-47be-9996-1b0d3467e38e",
   "metadata": {},
   "source": [
    "### Default `--min-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5e26b-12c9-44d4-82c4-153d832265a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --top-p 0.9 \\\n",
    "    --min-p 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40dbc8-0e28-4b8f-a45d-6bb77bd41e53",
   "metadata": {},
   "source": [
    "### Low `--min-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b26be-5553-4857-a023-cc565c7277c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --top-p 0.9 \\\n",
    "    --min-p 0.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b3064-8816-4f96-8a76-cb4fe9c0b3d2",
   "metadata": {},
   "source": [
    "### High `--min-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a03d3-19cb-48df-968e-b6654ba050af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --top-p 0.9 \\\n",
    "    --min-p 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a589750-bcec-4af3-b27c-4be372668e50",
   "metadata": {},
   "source": [
    "## Locally Typical Sampling\n",
    "\n",
    "Locally typical sampling, `--typical` promotes the generation of contextually coherent and diverse text by sampling tokens that are typical or expected based on the surrounding context. By setting the parameter $p$ between 0 and 1, you can control the balance between producing text that is locally coherent and diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac0446-5e83-403c-8b66-5533ce721567",
   "metadata": {},
   "source": [
    "### Default `--typical` example\n",
    "\n",
    "The default value of 1 disables locally typical sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1aabdf-5654-4a80-9d27-e3acd88d31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --typical 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f656c-8fe2-424e-9c95-2afc455ebf57",
   "metadata": {},
   "source": [
    "### Typical `--typical` example\n",
    "\n",
    "A value closer to 1 will promote more contextually coherent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d20679-77f5-4033-b9fd-c86d11687f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --typical 0.9\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb2804-e83c-492e-86cb-62b6836bc12f",
   "metadata": {},
   "source": [
    "### Low `--typical` example\n",
    "\n",
    "A value closer to 0 will promote more diverse tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35993f-0302-49a0-ac2a-0caa4296deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --typical 0.05\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed2635-b652-4e9a-89eb-ed87f7b2bdca",
   "metadata": {},
   "source": [
    "## Mirostat Sampling\n",
    "\n",
    "Mirostat is an algorithm that actively maintains the quality of generated text within a desired range during text generation. It aims to strike a balance between coherence and diversity, avoiding low-quality output caused by excessive repetition (boredom traps) or incoherence (confusion traps). To enable Mirostat sampling set `--mirostat` to 1 = Mirostat 1.0 or 2 = Mirostat 2.0. By default Mirostat sampling is disabled, `--mirostat 0`.\n",
    "\n",
    "The `--mirostat-lr` option sets the Mirostat learning rate (eta). The learning rate influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. The default value is `0.1`.\n",
    "\n",
    "The `--mirostat-ent` option sets the Mirostat target entropy (tau), which represents the desired perplexity value for the generated text. Adjusting the target entropy allows you to control the balance between coherence and diversity in the generated text. A lower value will result in more focused and coherent text, while a higher value will lead to more diverse and potentially less coherent text. The default value is `5.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433abca4-a249-463d-ba6b-43e916e56da1",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c2ca3-5401-4005-aa6f-bb27153312b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\" \"$PROMPT_FILE\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --file \"$2\" \\\n",
    "    --mirostat 2 \\\n",
    "    --mirostat-lr 0.05 \\\n",
    "    --mirostat-ent 3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da475842-8e5d-4eb7-bc63-cf96ac876357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
