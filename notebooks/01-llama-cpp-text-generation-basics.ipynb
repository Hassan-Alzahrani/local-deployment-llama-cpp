{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97500427-9c68-4273-bcab-b0a64ff18a68",
   "metadata": {},
   "source": [
    "# Text Generation Basics\n",
    "\n",
    "The following options allow you to control the text generation process and fine-tune the diversity, creativity, and quality of the generated text according to your needs. By adjusting these options and experimenting with different combinations of values, you can find the best settings for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d7fcc-b392-47b5-b6ab-7eccee6fc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"../models/gemma-1.1-7b-it.Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d0866-b4ff-43a8-94c2-9029c0abf4d4",
   "metadata": {},
   "source": [
    "## Random Number Generator (RNG) Seed\n",
    "\n",
    "The RNG seed is used to initialize the random number generator that influences the text generation process. By setting a specific value for `--seed` you can obtain consistent and reproducible results across multiple runs with the same input and settings. This can be helpful for testing, debugging, or comparing the effects of different options on the generated text to see when they diverge. If the seed is set to a value less than 0, a random seed will be used, which will result in different outputs on each run. The default value is -1 which will choose a random value for `--seed`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a676c8-5d90-425f-a900-b4b0694f38f8",
   "metadata": {},
   "source": [
    "### Random `--seed` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352f9d0-3861-4e6c-9b6c-1e3fb58ca8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --seed -1 \\\n",
    "    --color \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ecde3-e4ac-4983-a8cf-226bc70eef1d",
   "metadata": {},
   "source": [
    "### Fixed `--seed` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed26f2-7f4c-459b-93f6-782f8d4321b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --seed 42 \\\n",
    "    --color \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89e410-0143-4055-9639-4f3a2387930c",
   "metadata": {},
   "source": [
    "## Number of Tokens to Predict\n",
    "\n",
    "The `-n N, --predict N` (default: -1) controls the number of tokens the model generates in response to the input prompt. By adjusting this value, you can influence the length of the generated text. A higher value will result in longer text, while a lower value will produce shorter text.\n",
    "\n",
    "Even though all models have a finite context window, a value of -1 will enable *infinite* text generation. How? When the context window is full, some of the earlier tokens (half of the tokens after `--keep`) will be discarded. The context must then be re-evaluated before generation can resume. On large models and/or large context windows, this can result in a significant pause in output. If the output delay is undesirable, a value of -2 will stop generation immediately when the context is filled.\n",
    "\n",
    "It is important to note that the generated text may be shorter than the specified number of tokens if an End-of-Sequence (EOS) token or a reverse prompt is encountered. In interactive mode, text generation will pause and control will be returned to the user. In non-interactive mode, the program will end. In both cases, the text generation may stop before reaching the specified `--predict` value. If you want the model to keep going without ever producing End-of-Sequence on its own, you can use the `--ignore-eos` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d133e58-15ff-4119-aaef-4b59256e26fb",
   "metadata": {},
   "source": [
    "### Basic `--predict` example\n",
    "\n",
    "```bash\n",
    "llama-cli --model \"$MODEL\" --predict 10 --prompt \"What is the meaning of life?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12486c3-cdb9-47bb-b9a7-c3f65df82a64",
   "metadata": {},
   "source": [
    "### \"until context filled\" text generation example\n",
    "\n",
    "```bash\n",
    "llama-cli --model \"$MODEL\" --ctx-size 10 --predict -2 --prompt \"What is the meaning of life?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e90bd-deac-4bf7-a2c3-fa6a402a1592",
   "metadata": {},
   "source": [
    "### \"Infinite\" text generation example\n",
    "\n",
    "```bash\n",
    "llama-cli --model \"$MODEL\" --ctx-size 10 --predict -1 --prompt \"What is the meaning of life?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569217e-1faa-4cd6-92dc-a2b9fdd77af5",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "-   `--temp N`: Adjust the randomness of the generated text (default: 0.8).\n",
    "\n",
    "Temperature is a hyperparameter that controls the randomness of the generated text. It affects the probability distribution of the model's output tokens. A higher temperature (e.g., 1.5) makes the output more random and creative, while a lower temperature (e.g., 0.5) makes the output more focused, deterministic, and conservative. The default value is 0.8, which provides a balance between randomness and determinism. At the extreme, a temperature of 0 will always pick the most likely next token, leading to identical outputs in each run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ec8d2-e59a-411b-9937-ea72d38cb8a0",
   "metadata": {},
   "source": [
    "### Default `--temp` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc1720-8fd5-4d4c-bd4d-46f0cced0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli --model \"$1\" --temp 0.8 --color --file ../prompts/engaging-twitter-thread.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba68e0e-5a0b-4ce9-bbd2-5b3346498ecc",
   "metadata": {},
   "source": [
    "### Low `--temp` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aaa2b5-c8de-48d4-b52c-afa2d1f5493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli --model \"$1\" --temp 0.4 --color --file ../prompts/engaging-twitter-thread.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b25125-44be-45d5-96c4-1816857fb59e",
   "metadata": {},
   "source": [
    "### High `--temp` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f583ef51-1c95-4f29-b85b-aff440d87fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli --model \"$1\" --temp 1.4 --color --file ../prompts/engaging-twitter-thread.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fca20a-29da-4883-847d-943cd29a4c78",
   "metadata": {},
   "source": [
    "## Repeat Penalty\n",
    "\n",
    "The `--repeat-penalty` option helps prevent the model from generating repetitive or monotonous text. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. The default value is 1 (which means no penalty).\n",
    "\n",
    "The `--repeat-last-n` option controls the number of tokens in the history to consider for penalizing repetition. A larger value will look further back in the generated text to prevent repetitions, while a smaller value will only consider recent tokens. A value of 0 disables the penalty, and a value of -1 sets the number of tokens considered equal to the context size, `--ctx-size`. The default value is 64. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc3167-4d73-4136-94d6-685d0f753907",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --repeat-penalty 1.5 \\\n",
    "    --repeat-last-n 128 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd40af-3201-4d6b-96b2-a65e9a431689",
   "metadata": {},
   "source": [
    "## Top-K Sampling\n",
    "\n",
    "Top-k sampling is a text generation method that selects the next token only from the `--top-k` most likely tokens predicted by the model. It helps reduce the risk of generating low-probability or nonsensical tokens, but it may also limit the diversity of the output. A higher value for top-k (e.g., 100) will consider more tokens and lead to more diverse text, while a lower value (e.g., 10) will focus on the most probable tokens and generate more conservative text. The default value is 40.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7845a0e-f8a1-41d1-b265-6b99da840de0",
   "metadata": {},
   "source": [
    "### Default `--top-k` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ee26a-2fe2-42bb-9bcc-bfe143e6e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-k 40 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba89dd3-9f88-4fd7-85e8-57d6139b8c9b",
   "metadata": {},
   "source": [
    "### Low `--top-k` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec410a-818d-4a2e-aea2-e84231d744b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-k 10 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66035e2-502f-4d74-982e-34d2357774c7",
   "metadata": {},
   "source": [
    "### High `--top-k` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97acf190-4394-4582-aae8-669bdb361bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-k 100 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592f98d-9a51-458b-b8c8-5171a85cddf6",
   "metadata": {},
   "source": [
    "## Top-P Sampling\n",
    "\n",
    "Top-p sampling, `top-p`, also known as nucleus sampling, is another text generation method that selects the next token from a subset of tokens that together have a cumulative probability of at least p. This method provides a balance between diversity and quality by considering both the probabilities of tokens and the number of tokens to sample from. A higher value for top-p (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. The default value is 0.9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4215bdb-cdef-4d9e-a875-f41c07c7131a",
   "metadata": {},
   "source": [
    "### Default `--top-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd5ac7-fef8-4449-a75c-dd6918826936",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-p 0.9 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b65992-578b-4b0e-a7c9-a023257ffca8",
   "metadata": {},
   "source": [
    "### Low `--top-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c1bfc-ac22-4fa8-8c57-6945016dafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-p 0.5 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88066c66-c556-4c51-b016-431fb4532fd8",
   "metadata": {},
   "source": [
    "### High `--top-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ba3c0-8646-4597-8d92-ea3dc7c7f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-p 0.95 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf85b2e-c110-4dd7-84c6-952048fb55c4",
   "metadata": {},
   "source": [
    "## Min-P Sampling\n",
    "\n",
    "The `--min-p` sampling method sets a minimum base probability threshold for token selection and aims to ensure a balance of quality and variety in the generated text. The `--min-p` method was designed as an alternative to `--top-p`. The parameter $p$ represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with $p=0.05$ and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. The default value is 0.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193778cd-271f-47be-9996-1b0d3467e38e",
   "metadata": {},
   "source": [
    "### Default `--min-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5e26b-12c9-44d4-82c4-153d832265a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-p 0.9 \\\n",
    "    --min-p 0.1 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40dbc8-0e28-4b8f-a45d-6bb77bd41e53",
   "metadata": {},
   "source": [
    "### Low `--min-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b26be-5553-4857-a023-cc565c7277c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-p 0.9 \\\n",
    "    --min-p 0.05 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b3064-8816-4f96-8a76-cb4fe9c0b3d2",
   "metadata": {},
   "source": [
    "### High `--min-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a03d3-19cb-48df-968e-b6654ba050af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-p 0.9 \\\n",
    "    --min-p 0.2 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a589750-bcec-4af3-b27c-4be372668e50",
   "metadata": {},
   "source": [
    "## Locally Typical Sampling\n",
    "\n",
    "Locally typical sampling, `--typical` promotes the generation of contextually coherent and diverse text by sampling tokens that are typical or expected based on the surrounding context. By setting the parameter $p$ between 0 and 1, you can control the balance between producing text that is locally coherent and diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac0446-5e83-403c-8b66-5533ce721567",
   "metadata": {},
   "source": [
    "### Default `--typical` example\n",
    "\n",
    "The default value of 1 disables locally typical sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1aabdf-5654-4a80-9d27-e3acd88d31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --typical 1.0 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f656c-8fe2-424e-9c95-2afc455ebf57",
   "metadata": {},
   "source": [
    "### Typical `--typical` example\n",
    "\n",
    "A value closer to 1 will promote more contextually coherent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d20679-77f5-4033-b9fd-c86d11687f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --typical 0.9 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb2804-e83c-492e-86cb-62b6836bc12f",
   "metadata": {},
   "source": [
    "### Low `--typical` example\n",
    "\n",
    "A `--typical` value closer to 0 will promote more diverse tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35993f-0302-49a0-ac2a-0caa4296deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --typical 0.25 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed2635-b652-4e9a-89eb-ed87f7b2bdca",
   "metadata": {},
   "source": [
    "## Mirostat Sampling\n",
    "\n",
    "Mirostat is an algorithm that actively maintains the quality of generated text within a desired range during text generation. It aims to strike a balance between coherence and diversity, avoiding low-quality output caused by excessive repetition (boredom traps) or incoherence (confusion traps). To enable Mirostat sampling set `--mirostat` to 1 = Mirostat 1.0 or 2 = Mirostat 2.0. By default Mirostat sampling is disabled, `--mirostat 0`.\n",
    "\n",
    "The `--mirostat-lr` option sets the Mirostat learning rate (eta). The learning rate influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. The default value is `0.1`.\n",
    "\n",
    "The `--mirostat-ent` option sets the Mirostat target entropy (tau), which represents the desired perplexity value for the generated text. Adjusting the target entropy allows you to control the balance between coherence and diversity in the generated text. A lower value will result in more focused and coherent text, while a higher value will lead to more diverse and potentially less coherent text. The default value is `5.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433abca4-a249-463d-ba6b-43e916e56da1",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c2ca3-5401-4005-aa6f-bb27153312b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --mirostat 2 \\\n",
    "    --mirostat-lr 0.05 \\\n",
    "    --mirostat-ent 3.0 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da475842-8e5d-4eb7-bc63-cf96ac876357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6256957-6043-4bba-8f0c-92b199790d47",
   "metadata": {},
   "source": [
    "## 6. Performance Tuning and Memory Options\n",
    "\n",
    "These options help improve the performance and memory usage of the LLaMA models. By adjusting these settings, you can fine-tune the model's behavior to better suit your system's capabilities and achieve optimal performance for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a048cd5-2e6c-4c57-a667-ad78942fd660",
   "metadata": {},
   "source": [
    "### Number of Threads\n",
    "\n",
    "-   `-t N, --threads N`: Set the number of threads to use during generation. For optimal performance, it is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Using the correct number of threads can greatly improve performance.\n",
    "-   `-tb N, --threads-batch N`: Set the number of threads to use during batch and prompt processing. In some systems, it is beneficial to use a higher number of threads during batch processing than during generation. If not specified, the number of threads used for batch processing will be the same as the number of threads used for generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350ba86-1aa3-40da-a7de-2902a29c4851",
   "metadata": {},
   "source": [
    "### Mlock\n",
    "\n",
    "-   `--mlock`: Lock the model in memory, preventing it from being swapped out when memory-mapped. This can improve performance but trades away some of the advantages of memory-mapping by requiring more RAM to run and potentially slowing down load times as the model loads into RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c67d2-1bdf-4a0f-afe4-6ec6b4ad966e",
   "metadata": {},
   "source": [
    "### No Memory Mapping\n",
    "\n",
    "-   `--no-mmap`: Do not memory-map the model. By default, models are mapped into memory, which allows the system to load only the necessary parts of the model as needed. However, if the model is larger than your total amount of RAM or if your system is low on available memory, using mmap might increase the risk of pageouts, negatively impacting performance. Disabling mmap results in slower load times but may reduce pageouts if you're not using `--mlock`. Note that if the model is larger than the total amount of RAM, turning off mmap would prevent the model from loading at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afdf19c-7ab4-401c-aa0c-30390ca7d80b",
   "metadata": {},
   "source": [
    "### NUMA support\n",
    "\n",
    "-   `--numa distribute`: Pin an equal proportion of the threads to the cores on each NUMA node. This will spread the load amongst all cores on the system, utilitizing all memory channels at the expense of potentially requiring memory to travel over the slow links between nodes.\n",
    "-   `--numa isolate`: Pin all threads to the NUMA node that the program starts on. This limits the number of cores and amount of memory that can be used, but guarantees all memory access remains local to the NUMA node.\n",
    "-   `--numa numactl`: Pin threads to the CPUMAP that is passed to the program by starting it with the numactl utility. This is the most flexible mode, and allow arbitrary core usage patterns, for example a map that uses all the cores on one NUMA nodes, and just enough cores on a second node to saturate the inter-node memory bus.\n",
    "\n",
    " These flags attempt optimizations that help on some systems with non-uniform memory access. This currently consists of one of the above strategies, and disabling prefetch and readahead for mmap. The latter causes mapped pages to be faulted in on first access instead of all at once, and in combination with pinning threads to NUMA nodes, more of the pages end up on the NUMA node where they are used. Note that if the model is already in the system page cache, for example because of a previous run without this option, this will have little effect unless you drop the page cache first. This can be done by rebooting the system or on Linux by writing '3' to '/proc/sys/vm/drop_caches' as root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e29fe2-d43b-4289-b1e2-a10444a7f2c5",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "\n",
    "-   `-b N, --batch-size N`: Set the batch size for prompt processing (default: `2048`). This large batch size benefits users who have BLAS installed and enabled it during the build. If you don't have BLAS enabled (\"BLAS=0\"), you can use a smaller number, such as 8, to see the prompt progress as it's evaluated in some situations.\n",
    "\n",
    "- `-ub N`, `--ubatch-size N`: physical maximum batch size. This is for pipeline parallelization. Default: `512`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9386e12-59d5-41f2-b8c2-6f5152e7d9d4",
   "metadata": {},
   "source": [
    "### Prompt Caching\n",
    "\n",
    "-   `--prompt-cache FNAME`: Specify a file to cache the model state after the initial prompt. This can significantly speed up the startup time when you're using longer prompts. The file is created during the first run and is reused and updated in subsequent runs. **Note**: Restoring a cached prompt does not imply restoring the exact state of the session at the point it was saved. So even when specifying a specific seed, you are not guaranteed to get the same sequence of tokens as the original generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b38173-24ac-4d49-94cf-5e54e2cf7724",
   "metadata": {},
   "source": [
    "### Grammars & JSON schemas\n",
    "\n",
    "-   `--grammar GRAMMAR`, `--grammar-file FILE`: Specify a grammar (defined inline or in a file) to constrain model output to a specific format. For example, you could force the model to output JSON or to speak only in emojis. See the [GBNF guide](../../grammars/README.md) for details on the syntax.\n",
    "\n",
    "-   `--json-schema SCHEMA`: Specify a [JSON schema](https://json-schema.org/) to constrain model output to (e.g. `{}` for any JSON object, or `{\"items\": {\"type\": \"string\", \"minLength\": 10, \"maxLength\": 100}, \"minItems\": 10}` for a JSON array of strings with size constraints). If a schema uses external `$ref`s, you should use `--grammar \"$( python examples/json_schema_to_grammar.py myschema.json )\"` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f1f2e-870f-418a-871d-f5d4527ffe9e",
   "metadata": {},
   "source": [
    "## 6. Additional Options\n",
    "\n",
    "These options provide extra functionality and customization when running the LLaMA models:\n",
    "\n",
    "-   `-h, --help`: Display a help message showing all available options and their default values. This is particularly useful for checking the latest options and default values, as they can change frequently, and the information in this document may become outdated.\n",
    "-   `--verbose-prompt`: Print the prompt before generating text.\n",
    "-   `-mg i, --main-gpu i`: When using multiple GPUs this option controls which GPU is used for small tensors for which the overhead of splitting the computation across all GPUs is not worthwhile. The GPU in question will use slightly more VRAM to store a scratch buffer for temporary results. By default GPU 0 is used.\n",
    "-   `-ts SPLIT, --tensor-split SPLIT`: When using multiple GPUs this option controls how large tensors should be split across all GPUs. `SPLIT` is a comma-separated list of non-negative values that assigns the proportion of data that each GPU should get in order. For example, \"3,2\" will assign 60% of the data to GPU 0 and 40% to GPU 1. By default the data is split in proportion to VRAM but this may not be optimal for performance.\n",
    "-   `--lora FNAME`: Apply a LoRA (Low-Rank Adaptation) adapter to the model (implies --no-mmap). This allows you to adapt the pretrained model to specific tasks or domains.\n",
    "-   `--lora-base FNAME`: Optional model to use as a base for the layers modified by the LoRA adapter. This flag is used in conjunction with the `--lora` flag, and specifies the base model for the adaptation.\n",
    "-   `-hfr URL --hf-repo URL`: The url to the Hugging Face model repository. Used in conjunction with `--hf-file` or `-hff`. The model is downloaded and stored in the file provided by `-m` or `--model`. If `-m` is not provided, the model is auto-stored in the path specified by the `LLAMA_CACHE` environment variable  or in an OS-specific local cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b88e2e-57f8-4d5b-b752-8671cabed28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
