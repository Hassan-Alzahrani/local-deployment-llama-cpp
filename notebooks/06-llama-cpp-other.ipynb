{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b5052c-f030-4db4-a05e-3396fff3d1e5",
   "metadata": {},
   "source": [
    "## Contributing\n",
    "\n",
    "- Contributors can open PRs\n",
    "- Collaborators can push to branches in the `llama.cpp` repo and merge PRs into the `master` branch\n",
    "- Collaborators will be invited based on contributions\n",
    "- Any help with managing issues and PRs is very appreciated!\n",
    "- See [good first issues](https://github.com/ggerganov/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) for tasks suitable for first contributions\n",
    "- Read the [CONTRIBUTING.md](CONTRIBUTING.md) for more information\n",
    "- Make sure to read this: [Inference at the edge](https://github.com/ggerganov/llama.cpp/discussions/205)\n",
    "- A bit of backstory for those who are interested: [Changelog podcast](https://changelog.com/podcast/532)\n",
    "\n",
    "## Other documentations\n",
    "\n",
    "- [main (cli)](./examples/main/README.md)\n",
    "- [server](./examples/server/README.md)\n",
    "- [jeopardy](./examples/jeopardy/README.md)\n",
    "- [GBNF grammars](./grammars/README.md)\n",
    "\n",
    "**Development documentations**\n",
    "\n",
    "- [How to build](./docs/build.md)\n",
    "- [Running on Docker](./docs/docker.md)\n",
    "- [Build on Android](./docs/android.md)\n",
    "- [Performance troubleshooting](./docs/development/token_generation_performance_tips.md)\n",
    "- [GGML tips & tricks](https://github.com/ggerganov/llama.cpp/wiki/GGML-Tips-&-Tricks)\n",
    "\n",
    "**Seminal papers and background on the models**\n",
    "\n",
    "If your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:\n",
    "- LLaMA:\n",
    "    - [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)\n",
    "    - [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)\n",
    "- GPT-3\n",
    "    - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n",
    "- GPT-3.5 / InstructGPT / ChatGPT:\n",
    "    - [Aligning language models to follow instructions](https://openai.com/research/instruction-following)\n",
    "    - [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
