{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c8e642e-0c5e-47cc-961b-84b93e8e21f6",
   "metadata": {},
   "source": [
    "# llama-quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f198a9-571a-490d-b442-abc3b76e4d8d",
   "metadata": {},
   "source": [
    "## Step 1: Quantize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d055d7a-ee4c-4bc9-99c9-342b2ccddd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pughdr/Documents/Training/kaust-generative-ai/local-deployment-llama-cpp/env/bin/llama-quantize\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "which llama-quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7a40d8-0e46-4265-a56a-3971827d7c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main: build = 3865 (00b7317e)\n",
      "main: built with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: quantizing '../models/allenai-OLMo-7B-0724-Instruct-hf-bf16.gguf' to '../models/allenai-OLMo-7B-0724-Instruct-hf.Q4_K_M.gguf' as Q4_K_M\n",
      "llama_model_loader: loaded meta data with 31 key-value pairs and 226 tensors from ../models/allenai-OLMo-7B-0724-Instruct-hf-bf16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = olmo\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = OLMo 7B 0724 Instruct Hf\n",
      "llama_model_loader: - kv   3:                            general.version str              = 0724\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct-hf\n",
      "llama_model_loader: - kv   5:                           general.basename str              = OLMo\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                           general.datasets arr[str,3]       = [\"allenai/dolma\", \"allenai/tulu-v2-sf...\n",
      "llama_model_loader: - kv  10:                           olmo.block_count u32              = 32\n",
      "llama_model_loader: - kv  11:                        olmo.context_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                      olmo.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  13:                   olmo.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  14:                  olmo.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  15:               olmo.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  16:                        olmo.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  17:      olmo.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  19:          olmo.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = olmo\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,50304]   = [\"|||IP_ADDRESS|||\", \"<|padding|>\", \"...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,50304]   = [4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,50009]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"h e\", \"i n...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 50279\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 1\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{ eos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type bf16:  226 tensors\n",
      "[   1/ 226]                    token_embd.weight - [ 4096, 50304,     1,     1], type =   bf16, converting to q4_K .. size =   393.00 MiB ->   110.53 MiB\n",
      "[   2/ 226]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[   3/ 226]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[   4/ 226]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[   5/ 226]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   6/ 226]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   7/ 226]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   8/ 226]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[   9/ 226]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[  10/ 226]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  11/ 226]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  12/ 226]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  13/ 226]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  14/ 226]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  15/ 226]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  16/ 226]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[  17/ 226]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  18/ 226]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  19/ 226]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  20/ 226]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  21/ 226]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  22/ 226]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  23/ 226]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  24/ 226]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  25/ 226]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  26/ 226]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  27/ 226]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[  28/ 226]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  29/ 226]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  30/ 226]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  31/ 226]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  32/ 226]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  33/ 226]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  34/ 226]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  35/ 226]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  36/ 226]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  37/ 226]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  38/ 226]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  39/ 226]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  40/ 226]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  41/ 226]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  42/ 226]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  43/ 226]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  44/ 226]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  45/ 226]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  46/ 226]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  47/ 226]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  48/ 226]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[  49/ 226]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  50/ 226]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  51/ 226]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  52/ 226]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  53/ 226]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  54/ 226]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  55/ 226]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  56/ 226]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  57/ 226]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  58/ 226]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  59/ 226]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  60/ 226]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  61/ 226]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  62/ 226]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  63/ 226]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  64/ 226]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  65/ 226]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  66/ 226]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  67/ 226]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  68/ 226]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  69/ 226]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[  70/ 226]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  71/ 226]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  72/ 226]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  73/ 226]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  74/ 226]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  75/ 226]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  76/ 226]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  77/ 226]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  78/ 226]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  79/ 226]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  80/ 226]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  81/ 226]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  82/ 226]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  83/ 226]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  84/ 226]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  85/ 226]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  86/ 226]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[  87/ 226]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  88/ 226]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  89/ 226]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  90/ 226]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  91/ 226]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  92/ 226]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  93/ 226]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  94/ 226]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  95/ 226]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  96/ 226]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  97/ 226]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  98/ 226]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  99/ 226]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 100/ 226]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 101/ 226]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 102/ 226]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 103/ 226]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 104/ 226]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 105/ 226]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 106/ 226]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 107/ 226]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 108/ 226]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 109/ 226]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 110/ 226]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 111/ 226]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 112/ 226]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 113/ 226]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 114/ 226]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 115/ 226]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 116/ 226]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 117/ 226]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 118/ 226]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 119/ 226]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 120/ 226]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 121/ 226]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 122/ 226]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 123/ 226]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 124/ 226]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 125/ 226]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 126/ 226]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 127/ 226]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 128/ 226]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 129/ 226]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 130/ 226]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 131/ 226]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 132/ 226]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 133/ 226]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 134/ 226]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 135/ 226]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 136/ 226]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 137/ 226]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 138/ 226]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 139/ 226]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 140/ 226]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 141/ 226]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 142/ 226]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 143/ 226]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 144/ 226]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 145/ 226]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 146/ 226]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 147/ 226]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 148/ 226]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 149/ 226]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 150/ 226]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 151/ 226]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 152/ 226]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 153/ 226]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 154/ 226]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 155/ 226]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 156/ 226]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 157/ 226]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 158/ 226]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 159/ 226]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 160/ 226]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 161/ 226]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 162/ 226]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 163/ 226]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 164/ 226]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 165/ 226]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 166/ 226]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 167/ 226]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 168/ 226]                        output.weight - [ 4096, 50304,     1,     1], type =   bf16, converting to q6_K .. size =   393.00 MiB ->   161.19 MiB\n",
      "[ 169/ 226]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 170/ 226]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 171/ 226]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 172/ 226]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 173/ 226]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 174/ 226]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 175/ 226]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 176/ 226]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 177/ 226]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 178/ 226]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 179/ 226]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 180/ 226]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 181/ 226]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 182/ 226]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 183/ 226]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 184/ 226]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 185/ 226]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 186/ 226]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 187/ 226]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 188/ 226]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 189/ 226]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 190/ 226]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 191/ 226]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 192/ 226]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 193/ 226]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 194/ 226]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 195/ 226]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 196/ 226]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 197/ 226]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 198/ 226]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 199/ 226]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 200/ 226]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 201/ 226]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 202/ 226]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 203/ 226]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 204/ 226]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 205/ 226]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 206/ 226]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 207/ 226]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 208/ 226]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 209/ 226]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 210/ 226]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 211/ 226]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 212/ 226]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 213/ 226]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 214/ 226]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 215/ 226]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 216/ 226]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 217/ 226]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 218/ 226]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 219/ 226]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 220/ 226]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 221/ 226]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 222/ 226]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =   bf16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 223/ 226]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 224/ 226]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 225/ 226]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 226/ 226]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "llama_model_quantize_internal: model size  = 13138.00 MB\n",
      "llama_model_quantize_internal: quant size  =  3989.10 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main: quantize time = 58443.00 ms\n",
      "main:    total time = 58443.00 ms\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "QUANTIZATION_TYPE=Q4_K_M\n",
    "llama-quantize \\\n",
    "    ../models/allenai-OLMo-7B-0724-Instruct-hf-bf16.gguf \\\n",
    "    ../models/allenai-OLMo-7B-0724-Instruct-hf.\"$QUANTIZATION_TYPE\".gguf \\\n",
    "    \"$QUANTIZATION_TYPE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3960f0-64c0-4eee-94d0-24d41c4e0d96",
   "metadata": {},
   "source": [
    "## Run the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b524023-693c-45b0-9711-b8496a103afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 31 key-value pairs and 226 tensors from ../models/allenai-OLMo-7B-0724-Instruct-hf.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = olmo\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = OLMo 7B 0724 Instruct Hf\n",
      "llama_model_loader: - kv   3:                            general.version str              = 0724\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct-hf\n",
      "llama_model_loader: - kv   5:                           general.basename str              = OLMo\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                           general.datasets arr[str,3]       = [\"allenai/dolma\", \"allenai/tulu-v2-sf...\n",
      "llama_model_loader: - kv  10:                           olmo.block_count u32              = 32\n",
      "llama_model_loader: - kv  11:                        olmo.context_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                      olmo.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  13:                   olmo.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  14:                  olmo.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  15:               olmo.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  16:                        olmo.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  17:      olmo.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:          olmo.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = olmo\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,50304]   = [\"|||IP_ADDRESS|||\", \"<|padding|>\", \"...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,50304]   = [4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,50009]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"h e\", \"i n...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 50279\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 1\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{ eos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 28\n",
      "llm_load_vocab: token to piece cache size = 0.2985 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = olmo\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50304\n",
      "llm_load_print_meta: n_merges         = 50009\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.89 B\n",
      "llm_load_print_meta: model size       = 3.90 GiB (4.86 BPW) \n",
      "llm_load_print_meta: general.name     = OLMo 7B 0724 Instruct Hf\n",
      "llm_load_print_meta: BOS token        = 11 '*'\n",
      "llm_load_print_meta: EOS token        = 50279 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 1 '<|padding|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 50279 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 50279 '<|endoftext|>'\n",
      "llm_load_print_meta: max token length = 1024\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   110.53 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3878.58 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 965\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 42\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is the sky blue? Because the Sun's white light is made up of many colors that scatter and mix in the air. The blue color of the sky is most intense over the ocean because of the greater distance between the Sun and the shore.\n",
      "\n",
      "Why is the sky purple? The Sun's white light passes through the atmosphere and scatters the blue light. The remaining colors, including red and orange, are reflected in the sky. This causes the sky to appear a soft purple color.\n",
      "\n",
      "Why is the sky pink? At sunrise and sunset, the Sun is below the horizon. The Sun's light travels through a greater amount of the Earth's atmosphere, and the blue light is scattered more than the other colors. This leaves the red and orange colors in the sky, which create the pink color.\n",
      "\n",
      "Why is the sky green? During a solar eclipse, the Moon's shadow blocks the Sun's light. The Moon's shadow contains a high percentage of blue light, which passes through the atmosphere. When the Sun's light travels back through the atmosphere, it scatters the blue light into the sky, creating a green glow.\n",
      "\n",
      "Why is the sky blue at noon? The Earth's atmosphere is full of tiny particles and gas molecules. When the Sun's white light shines through the atmosphere, the blue color is scattered more than the other colors. This is because blue light travels in a straight line, while other colors scatter at different angles. The blue color is most intense in the sky during the middle of the day.\n",
      "\n",
      "Why is the sky black at night? The Earth's atmosphere blocks the Sun's light during the night. The sky appears black because there is no sunlight to illuminate it. However, the Moon's light can shine through the atmosphere and create a soft glow in the sky.\n",
      "\n",
      "Why is the sky blue at sunrise and sunset? The Sun's light travels through more of the Earth's atmosphere during sunrise and sunset. The blue color is scattered more than the other colors because blue light scatters at a different angle than other colors. This creates a soft blue glow in the sky during the morning and evening hours.\n",
      "\n",
      "Why is the sky purple at night? The Earth's atmosphere blocks the Sun's light during the night, and the Moon's light creates a soft glow in the sky. The remaining colors, including red and orange, are reflected in the sky to create a soft purple color.\n",
      "\n",
      "Why is the sky blue during a thunderstorm? Thunderstorms produce clouds that contain water droplets and ice crystals. When the Sun's white light shines through the clouds, the water droplets scatter the blue light more than the other colors. This creates a blue glow in the sky during a thunderstorm.\n",
      "\n",
      "Why is the sky purple during a thunderstorm? Thunderstorms produce clouds that contain water droplets and ice crystals. When the Sun's white light shines through the clouds, the water droplets scatter the red and orange colors more than the blue color. This creates a soft purple glow in the sky during a thunderstorm.\n",
      "\n",
      "Why is the sky blue in a clear sky? The Earth's atmosphere is full of tiny particles and gas molecules. When the Sun's white light shines through the atmosphere, the blue color is scattered more than the other colors. This creates a soft blue glow in the sky, even in a clear sky.\n",
      "\n",
      "\n",
      "Interrupted by user\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      18.78 ms /   695 runs   (    0.03 ms per token, 37003.51 tokens per second)\n",
      "llama_perf_context_print:        load time =    2673.54 ms\n",
      "llama_perf_context_print: prompt eval time =     219.39 ms /     6 tokens (   36.56 ms per token,    27.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36207.29 ms /   688 runs   (   52.63 ms per token,    19.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   36507.27 ms /   694 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "llama-cli \\\n",
    "    --model ../models/allenai-OLMo-7B-0724-Instruct-hf.Q4_K_M.gguf \\\n",
    "    --prompt \"Why is the sky blue?\" \\\n",
    "    --seed 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02672deb-9265-4f0a-a0a3-4cd73505d8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- common params -----\n",
      "\n",
      "-h,    --help, --usage                  print usage and exit\n",
      "--version                               show version and build info\n",
      "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
      "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
      "                                        (env: LLAMA_ARG_THREADS)\n",
      "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
      "                                        same as --threads)\n",
      "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
      "                                        (default: \"\")\n",
      "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
      "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
      "--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
      "                                        (default: 0)\n",
      "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
      "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
      "                                        (default: same as --cpu-mask)\n",
      "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
      "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
      "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
      "                                        (default: 0)\n",
      "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
      "-c,    --ctx-size N                     size of the prompt context (default: 0, 0 = loaded from model)\n",
      "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
      "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
      "                                        context filled)\n",
      "                                        (env: LLAMA_ARG_N_PREDICT)\n",
      "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
      "                                        (env: LLAMA_ARG_BATCH)\n",
      "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
      "                                        (env: LLAMA_ARG_UBATCH)\n",
      "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
      "                                        all)\n",
      "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
      "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
      "-p,    --prompt PROMPT                  prompt to start generation with\n",
      "                                        if -cnv is set, this will be used as system prompt\n",
      "--no-perf                               disable internal libllama performance timings (default: false)\n",
      "                                        (env: LLAMA_ARG_NO_PERF)\n",
      "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
      "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
      "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
      "--no-escape                             do not process escape sequences\n",
      "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
      "                                        the model\n",
      "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
      "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
      "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
      "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
      "                                        model)\n",
      "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
      "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
      "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
      "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
      "                                        context size)\n",
      "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
      "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
      "                                        interpolation)\n",
      "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
      "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
      "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
      "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
      "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
      "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
      "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
      "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
      "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
      "-gaw,  --grp-attn-w N                   group-attention width (default: 512.0)\n",
      "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
      "-dkvc, --dump-kv-cache                  verbose print of the KV cache\n",
      "-nkvo, --no-kv-offload                  disable KV offload\n",
      "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
      "-ctk,  --cache-type-k TYPE              KV cache data type for K (default: f16)\n",
      "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
      "-ctv,  --cache-type-v TYPE              KV cache data type for V (default: f16)\n",
      "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
      "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: -1.0, < 0 - disabled)\n",
      "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
      "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
      "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
      "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
      "                                        (env: LLAMA_ARG_MLOCK)\n",
      "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
      "                                        using mlock)\n",
      "                                        (env: LLAMA_ARG_NO_MMAP)\n",
      "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
      "                                        - distribute: spread execution evenly over all nodes\n",
      "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
      "                                        started on\n",
      "                                        - numactl: use the CPU map provided by numactl\n",
      "                                        if run without this previously, it is recommended to drop the system\n",
      "                                        page cache before using this\n",
      "                                        see https://github.com/ggerganov/llama.cpp/issues/1437\n",
      "                                        (env: LLAMA_ARG_NUMA)\n",
      "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
      "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
      "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
      "                                        - none: use one GPU only\n",
      "                                        - layer (default): split layers and KV across GPUs\n",
      "                                        - row: split rows across GPUs\n",
      "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
      "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
      "                                        proportions, e.g. 3,1\n",
      "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
      "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
      "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
      "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
      "--check-tensors                         check model tensor data for invalid values (default: false)\n",
      "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
      "                                        multiple times.\n",
      "                                        types: int, float, bool, str. example: --override-kv\n",
      "                                        tokenizer.ggml.add_bos_token=bool:false\n",
      "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
      "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
      "                                        multiple adapters)\n",
      "--control-vector FNAME                  add a control vector\n",
      "                                        note: this argument can be repeated to add multiple control vectors\n",
      "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
      "                                        note: this argument can be repeated to add multiple scaled control\n",
      "                                        vectors\n",
      "--control-vector-layer-range START END\n",
      "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
      "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
      "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
      "                                        (env: LLAMA_ARG_MODEL)\n",
      "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
      "                                        (env: LLAMA_ARG_MODEL_URL)\n",
      "-hfr,  --hf-repo REPO                   Hugging Face model repository (default: unused)\n",
      "                                        (env: LLAMA_ARG_HF_REPO)\n",
      "-hff,  --hf-file FILE                   Hugging Face model file (default: unused)\n",
      "                                        (env: LLAMA_ARG_HF_FILE)\n",
      "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
      "                                        variable)\n",
      "                                        (env: HF_TOKEN)\n",
      "-ld,   --logdir LOGDIR                  path under which to save YAML logs (no logging if unset)\n",
      "--log-disable                           Log disable\n",
      "--log-file FNAME                        Log to file\n",
      "--log-colors                            Enable colored logging\n",
      "                                        (env: LLAMA_LOG_COLORS)\n",
      "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
      "                                        debugging)\n",
      "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
      "                                        ignored.\n",
      "                                        (env: LLAMA_LOG_VERBOSITY)\n",
      "--log-prefix                            Enable prefx in log messages\n",
      "                                        (env: LLAMA_LOG_PREFIX)\n",
      "--log-timestamps                        Enable timestamps in log messages\n",
      "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
      "\n",
      "\n",
      "----- sampling params -----\n",
      "\n",
      "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
      "                                        ';'\n",
      "                                        (default: top_k;tfs_z;typ_p;top_p;min_p;temperature)\n",
      "-s,    --seed SEED                      RNG seed (default: 4294967295, use random seed for 4294967295)\n",
      "--sampling-seq SEQUENCE                 simplified sequence for samplers that will be used (default: kfypmt)\n",
      "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
      "                                        --logit-bias EOS-inf)\n",
      "--penalize-nl                           penalize newline tokens (default: false)\n",
      "--temp N                                temperature (default: 0.8)\n",
      "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
      "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
      "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
      "--tfs N                                 tail free sampling, parameter z (default: 1.0, 1.0 = disabled)\n",
      "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
      "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
      "                                        = ctx_size)\n",
      "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
      "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
      "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
      "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
      "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
      "--mirostat N                            use Mirostat sampling.\n",
      "                                        Top K, Nucleus, Tail Free and Locally Typical samplers are ignored if\n",
      "                                        used.\n",
      "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
      "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
      "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
      "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
      "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
      "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
      "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
      "                                        dir) (default: '')\n",
      "--grammar-file FNAME                    file to read grammar from\n",
      "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
      "                                        `{}` for any JSON object\n",
      "                                        For schemas w/ external $refs, use --grammar +\n",
      "                                        example/json_schema_to_grammar.py instead\n",
      "\n",
      "\n",
      "----- example-specific params -----\n",
      "\n",
      "--no-display-prompt                     don't print prompt at generation (default: false)\n",
      "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
      "                                        (default: false)\n",
      "--no-context-shift                      disables context shift on inifinite text generation (default:\n",
      "                                        disabled)\n",
      "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
      "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
      "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
      "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
      "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
      "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
      "-sp,   --special                        special tokens output enabled (default: false)\n",
      "-cnv,  --conversation                   run in conversation mode:\n",
      "                                        - does not print special tokens and suffix/prefix\n",
      "                                        - interactive mode is also enabled\n",
      "                                        (default: false)\n",
      "-i,    --interactive                    run in interactive mode (default: false)\n",
      "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
      "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
      "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
      "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
      "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
      "--no-warmup                             skip warming up the model with an empty run\n",
      "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
      "                                        metadata)\n",
      "                                        if suffix/prefix are specified, template will be disabled\n",
      "                                        only commonly used templates are accepted:\n",
      "                                        https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template\n",
      "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
      "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
      "                                        consoles\n",
      "\n",
      "example usage:\n",
      "\n",
      "  text generation:     llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128\n",
      "\n",
      "  chat (conversation): llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!llama-cli -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f087c08-13e3-4ae5-8063-a398a2fc8aa1",
   "metadata": {},
   "source": [
    "When running the larger models, make sure you have enough disk space to store all the intermediate files.\n",
    "\n",
    "## Memory/Disk Requirements\n",
    "\n",
    "As the models are currently fully loaded into memory, you will need adequate disk space to save them and sufficient RAM to load them. At the moment, memory and disk requirements are the same.\n",
    "\n",
    "| Model | Original size | Quantized size (Q4_0) |\n",
    "|------:|--------------:|----------------------:|\n",
    "|    7B |         13 GB |                3.9 GB |\n",
    "|   13B |         24 GB |                7.8 GB |\n",
    "|   30B |         60 GB |               19.5 GB |\n",
    "|   65B |        120 GB |               38.5 GB |\n",
    "\n",
    "## Quantization\n",
    "\n",
    "Several quantization methods are supported. They differ in the resulting model disk size and inference speed.\n",
    "\n",
    "The quantization formats `Q4_0_4_4`, `Q4_0_4_8` and `Q4_0_8_8` are block interleaved variants of the `Q4_0` format, providing a data layout that is better suited for specific implementations of optimized mulmat kernels. Since these formats differ only in data layout, they have the same quantized size as the `Q4_0` format.\n",
    "\n",
    "*(outdated)*\n",
    "\n",
    "| Model | Measure      |    F16 |   Q4_0 |   Q4_1 |   Q5_0 |   Q5_1 |   Q8_0 |\n",
    "|------:|--------------|-------:|-------:|-------:|-------:|-------:|-------:|\n",
    "|    7B | perplexity   | 5.9066 | 6.1565 | 6.0912 | 5.9862 | 5.9481 | 5.9070 |\n",
    "|    7B | file size    |  13.0G |   3.5G |   3.9G |   4.3G |   4.7G |   6.7G |\n",
    "|    7B | ms/tok @ 4th |    127 |     55 |     54 |     76 |     83 |     72 |\n",
    "|    7B | ms/tok @ 8th |    122 |     43 |     45 |     52 |     56 |     67 |\n",
    "|    7B | bits/weight  |   16.0 |    4.5 |    5.0 |    5.5 |    6.0 |    8.5 |\n",
    "|   13B | perplexity   | 5.2543 | 5.3860 | 5.3608 | 5.2856 | 5.2706 | 5.2548 |\n",
    "|   13B | file size    |  25.0G |   6.8G |   7.6G |   8.3G |   9.1G |    13G |\n",
    "|   13B | ms/tok @ 4th |      - |    103 |    105 |    148 |    160 |    131 |\n",
    "|   13B | ms/tok @ 8th |      - |     73 |     82 |     98 |    105 |    128 |\n",
    "|   13B | bits/weight  |   16.0 |    4.5 |    5.0 |    5.5 |    6.0 |    8.5 |\n",
    "\n",
    "- [k-quants](https://github.com/ggerganov/llama.cpp/pull/1684)\n",
    "- recent k-quants improvements and new i-quants\n",
    "  - [#2707](https://github.com/ggerganov/llama.cpp/pull/2707)\n",
    "  - [#2807](https://github.com/ggerganov/llama.cpp/pull/2807)\n",
    "  - [#4773 - 2-bit i-quants (inference)](https://github.com/ggerganov/llama.cpp/pull/4773)\n",
    "  - [#4856 - 2-bit i-quants (inference)](https://github.com/ggerganov/llama.cpp/pull/4856)\n",
    "  - [#4861 - importance matrix](https://github.com/ggerganov/llama.cpp/pull/4861)\n",
    "  - [#4872 - MoE models](https://github.com/ggerganov/llama.cpp/pull/4872)\n",
    "  - [#4897 - 2-bit quantization](https://github.com/ggerganov/llama.cpp/pull/4897)\n",
    "  - [#4930 - imatrix for all k-quants](https://github.com/ggerganov/llama.cpp/pull/4930)\n",
    "  - [#4951 - imatrix on the GPU](https://github.com/ggerganov/llama.cpp/pull/4957)\n",
    "  - [#4969 - imatrix for legacy quants](https://github.com/ggerganov/llama.cpp/pull/4969)\n",
    "  - [#4996 - k-qunats tuning](https://github.com/ggerganov/llama.cpp/pull/4996)\n",
    "  - [#5060 - Q3_K_XS](https://github.com/ggerganov/llama.cpp/pull/5060)\n",
    "  - [#5196 - 3-bit i-quants](https://github.com/ggerganov/llama.cpp/pull/5196)\n",
    "  - [quantization tuning](https://github.com/ggerganov/llama.cpp/pull/5320), [another one](https://github.com/ggerganov/llama.cpp/pull/5334), and [another one](https://github.com/ggerganov/llama.cpp/pull/5361)\n",
    "\n",
    "**Llama 2 7B**\n",
    "\n",
    "| Quantization | Bits per Weight (BPW) |\n",
    "|--------------|-----------------------|\n",
    "| Q2_K         | 3.35                  |\n",
    "| Q3_K_S       | 3.50                  |\n",
    "| Q3_K_M       | 3.91                  |\n",
    "| Q3_K_L       | 4.27                  |\n",
    "| Q4_K_S       | 4.58                  |\n",
    "| Q4_K_M       | 4.84                  |\n",
    "| Q5_K_S       | 5.52                  |\n",
    "| Q5_K_M       | 5.68                  |\n",
    "| Q6_K         | 6.56                  |\n",
    "\n",
    "**Llama 2 13B**\n",
    "\n",
    "Quantization | Bits per Weight (BPW)\n",
    "-- | --\n",
    "Q2_K | 3.34\n",
    "Q3_K_S | 3.48\n",
    "Q3_K_M | 3.89\n",
    "Q3_K_L | 4.26\n",
    "Q4_K_S | 4.56\n",
    "Q4_K_M | 4.83\n",
    "Q5_K_S | 5.51\n",
    "Q5_K_M | 5.67\n",
    "Q6_K | 6.56\n",
    "\n",
    "**Llama 2 70B**\n",
    "\n",
    "Quantization | Bits per Weight (BPW)\n",
    "-- | --\n",
    "Q2_K | 3.40\n",
    "Q3_K_S | 3.47\n",
    "Q3_K_M | 3.85\n",
    "Q3_K_L | 4.19\n",
    "Q4_K_S | 4.53\n",
    "Q4_K_M | 4.80\n",
    "Q5_K_S | 5.50\n",
    "Q5_K_M | 5.65\n",
    "Q6_K | 6.56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8faf9e7-97ab-456c-82c9-cb15e6bfe950",
   "metadata": {},
   "source": [
    "You can also use the [GGUF-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) space on Hugging Face to build your own quants without any setup.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
