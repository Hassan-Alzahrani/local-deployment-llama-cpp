{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422dc501-69bf-4105-b800-9f14c596c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from IPython.display import JSON, Markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0fe23f-3f19-433c-a4ee-d9e3710ce3e4",
   "metadata": {},
   "source": [
    "# LLaMA C++ HTTP Server Basics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e122ad2-6066-42b2-827a-6a59870a0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "which llama-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd095ac-8781-4d1a-b44d-69b68dee4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "llama-server --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a2fe2-bd19-4e38-91f7-204b30477580",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "To get started right away, open a terminal and run the following command, making sure to use the correct path for the model you have.\n",
    "\n",
    "```bash\n",
    "MODEL=\"./models/gemma-1.1-7b-it.Q4_K_M.gguf\"\n",
    "llama-server \\\n",
    "    --model $MODEL \\\n",
    "    --host localhost \\\n",
    "    --port 8080\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a2033-7202-49d7-959e-8b8ca17146d7",
   "metadata": {},
   "source": [
    "## Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1418266-d07d-4bca-84da-fd38cc9e0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://localhost:8080/health\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397c4a12-9944-483c-bba4-44038df85905",
   "metadata": {},
   "source": [
    "### Response Format\n",
    "\n",
    "- HTTP status code 503\n",
    "  - Body: `{\"error\": {\"code\": 503, \"message\": \"Loading model\", \"type\": \"unavailable_error\"}}`\n",
    "  - Explanation: the model is still being loaded.\n",
    "- HTTP status code 200\n",
    "  - Body: `{\"status\": \"ok\" }`\n",
    "  - Explanation: the model is successfully loaded and the server is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053136f9-92b5-4bb5-a43c-8256d01797bb",
   "metadata": {},
   "source": [
    "## Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e93c4a-010a-4f89-a43f-3d08a6d6dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    url=\"http://localhost:8080/completion\",\n",
    "    json={\n",
    "        \"prompt\": \"Why is the sky blue?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618154b2-d978-40cf-b4e5-19684aeadba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = json.loads(response.content)\n",
    "JSON(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90560bce-4a2a-4e50-9895-3ce148d0f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(json_response[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0927b2-e230-4a2a-ab71-0c60c8a9aac3",
   "metadata": {},
   "source": [
    "## Checking Server Global Properties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033ae62-8d5c-42f7-81af-3b406c281daa",
   "metadata": {},
   "source": [
    "This `/props` API endpoint allows you to get the current global settings for the server. By default, it is read-only: to make changes to global properties, you need to start server with the `--props` option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c6566-bcf7-482e-8f2a-51642811dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://localhost:8080/props\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da877c0a-87a8-4006-902f-b46d613794b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc8f79-0b28-48b4-8762-59bac87bf761",
   "metadata": {},
   "outputs": [],
   "source": [
    "_json_data = json.loads(response.content)\n",
    "JSON(_json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd864c0-95e9-49bb-82f6-2f4cc9571571",
   "metadata": {},
   "source": [
    "### Response Format\n",
    "- `system_prompt`: the default value for the model's system prompt (if any).\n",
    "- `default_generation_settings`: the default generation settings for the `/completion` endpoint, which has the same fields as the `generation_settings` response object from the `/completion` endpoint.\n",
    "- `total_slots`: the total number of slots for process requests (defined by `--parallel` option).\n",
    "- `chat_template`: the model's original Jinja2 prompt template (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc25134-91ac-4a98-9437-7c68c7f0fd8a",
   "metadata": {},
   "source": [
    "## Changing Server Global Properties \n",
    "\n",
    "To use the `/props` API endpoint POST method, you need to start server with `--props`.\n",
    "\n",
    "```bash\n",
    "MODEL=\"./models/gemma-1.1-7b-it.Q4_K_M.gguf\"\n",
    "llama-server \\\n",
    "    --model $MODEL \\\n",
    "    --host localhost \\\n",
    "    --port 8080 \\\n",
    "    --props\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa64f9-f476-469a-882c-00cb3a943ca2",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "If you launch your server using the `--metrics` option, then this will expose a [Prometheus-compatible](https://prometheus.io/) metrics exporter.\n",
    "\n",
    "```bash\n",
    "MODEL=\"./models/gemma-1.1-7b-it.Q4_K_M.gguf\"\n",
    "llama-server \\\n",
    "    --model $MODEL \\\n",
    "    --host localhost \\\n",
    "    --port 8080 \\\n",
    "    --metrics\n",
    "```\n",
    "\n",
    "### Available metrics:\n",
    "\n",
    "- `llamacpp:prompt_tokens_total`: Number of prompt tokens processed.\n",
    "- `llamacpp:tokens_predicted_total`: Number of generation tokens processed.\n",
    "- `llamacpp:prompt_tokens_seconds`: Average prompt throughput in tokens/s.\n",
    "- `llamacpp:predicted_tokens_seconds`: Average generation throughput in tokens/s.\n",
    "- `llamacpp:kv_cache_usage_ratio`: KV-cache usage. `1` means 100 percent usage.\n",
    "- `llamacpp:kv_cache_tokens`: KV-cache tokens.\n",
    "- `llamacpp:requests_processing`: Number of requests processing.\n",
    "- `llamacpp:requests_deferred`: Number of requests deferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d4564-3641-4f41-84dc-f42786f2ba9b",
   "metadata": {},
   "source": [
    "### Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603df12-9b85-4de7-bae1-bdeb4679555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    url=\"http://localhost:8080/metrics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b4b98-5604-4b97-968d-f9caeb466dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_metrics = (\n",
    "    response.content\n",
    "            .decode(\"utf-8\")\n",
    ")\n",
    "print(current_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724520e1-0812-4c87-816d-a1b612b12c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
