{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a29b5c-915c-4af6-977f-00f73d869a22",
   "metadata": {},
   "source": [
    "# LLaMA C++: Inference for [LLaMA](https://arxiv.org/abs/2302.13971) models (and others) in C/C++\n",
    "\n",
    "![llama](https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png)\n",
    "\n",
    "\n",
    "## Description\n",
    "\n",
    "The main goal of `llama.cpp` is to enable LLM inference with minimal setup and state-of-the-art performance on a wide\n",
    "variety of hardware - locally and in the cloud.\n",
    "\n",
    "- Plain C/C++ implementation without any dependencies\n",
    "- Apple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks\n",
    "- AVX, AVX2 and AVX512 support for x86 architectures\n",
    "- 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use\n",
    "- Custom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP)\n",
    "- Vulkan and SYCL backend support\n",
    "- CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity\n",
    "\n",
    "Since its [inception](https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022), the project has\n",
    "improved significantly thanks to many contributions. It is the main playground for developing new features for the\n",
    "[ggml](https://github.com/ggerganov/ggml) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620a0ef2-3f96-418d-9df7-bf50cc42cdad",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033f1afb-da82-4138-acec-54878917963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1138  100  1138    0     0   2779      0 --:--:-- --:--:-- --:--:--  2775\n",
      "100 2282M  100 2282M    0     0  10.6M      0  0:03:33  0:03:33 --:--:-- 11.4M\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../models/ggufs/\n",
    "URL=https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf\n",
    "curl --location --output ../models/ggufs/Phi-3-mini-4k-instruct-q4.gguf $URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b075f7a4-dd99-4016-8069-9154be8e1e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4685952\n",
      "-rw-r--r--  1 pughdr  KAUST\\Domain Users   2.2G Oct  2 15:11 Phi-3-mini-4k-instruct-q4.gguf\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls -lh ../models/ggufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "348a6d20-0fa7-4a7e-ae12-949165042124",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"../models/ggufs/Phi-3-mini-4k-instruct-q4.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008d3ea1-12b5-45f6-8425-7103909d0eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pughdr/Documents/Training/kaust-generative-ai/local-deployment-llama-cpp/env/bin/llama-cli\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "which llama-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9d4f79-e4b9-4983-9688-77a50da819fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- common params -----\n",
      "\n",
      "-h,    --help, --usage                  print usage and exit\n",
      "--version                               show version and build info\n",
      "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
      "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
      "                                        (env: LLAMA_ARG_THREADS)\n",
      "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
      "                                        same as --threads)\n",
      "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
      "                                        (default: \"\")\n",
      "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
      "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
      "--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
      "                                        (default: 0)\n",
      "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
      "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
      "                                        (default: same as --cpu-mask)\n",
      "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
      "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
      "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
      "                                        (default: 0)\n",
      "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
      "-c,    --ctx-size N                     size of the prompt context (default: 0, 0 = loaded from model)\n",
      "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
      "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
      "                                        context filled)\n",
      "                                        (env: LLAMA_ARG_N_PREDICT)\n",
      "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
      "                                        (env: LLAMA_ARG_BATCH)\n",
      "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
      "                                        (env: LLAMA_ARG_UBATCH)\n",
      "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
      "                                        all)\n",
      "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
      "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
      "-p,    --prompt PROMPT                  prompt to start generation with\n",
      "                                        if -cnv is set, this will be used as system prompt\n",
      "--no-perf                               disable internal libllama performance timings (default: false)\n",
      "                                        (env: LLAMA_ARG_NO_PERF)\n",
      "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
      "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
      "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
      "--no-escape                             do not process escape sequences\n",
      "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
      "                                        the model\n",
      "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
      "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
      "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
      "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
      "                                        model)\n",
      "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
      "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
      "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
      "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
      "                                        context size)\n",
      "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
      "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
      "                                        interpolation)\n",
      "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
      "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
      "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
      "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
      "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
      "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
      "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
      "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
      "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
      "-gaw,  --grp-attn-w N                   group-attention width (default: 512.0)\n",
      "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
      "-dkvc, --dump-kv-cache                  verbose print of the KV cache\n",
      "-nkvo, --no-kv-offload                  disable KV offload\n",
      "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
      "-ctk,  --cache-type-k TYPE              KV cache data type for K (default: f16)\n",
      "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
      "-ctv,  --cache-type-v TYPE              KV cache data type for V (default: f16)\n",
      "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
      "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: -1.0, < 0 - disabled)\n",
      "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
      "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
      "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
      "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
      "                                        (env: LLAMA_ARG_MLOCK)\n",
      "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
      "                                        using mlock)\n",
      "                                        (env: LLAMA_ARG_NO_MMAP)\n",
      "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
      "                                        - distribute: spread execution evenly over all nodes\n",
      "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
      "                                        started on\n",
      "                                        - numactl: use the CPU map provided by numactl\n",
      "                                        if run without this previously, it is recommended to drop the system\n",
      "                                        page cache before using this\n",
      "                                        see https://github.com/ggerganov/llama.cpp/issues/1437\n",
      "                                        (env: LLAMA_ARG_NUMA)\n",
      "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
      "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
      "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
      "                                        - none: use one GPU only\n",
      "                                        - layer (default): split layers and KV across GPUs\n",
      "                                        - row: split rows across GPUs\n",
      "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
      "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
      "                                        proportions, e.g. 3,1\n",
      "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
      "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
      "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
      "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
      "--check-tensors                         check model tensor data for invalid values (default: false)\n",
      "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
      "                                        multiple times.\n",
      "                                        types: int, float, bool, str. example: --override-kv\n",
      "                                        tokenizer.ggml.add_bos_token=bool:false\n",
      "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
      "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
      "                                        multiple adapters)\n",
      "--control-vector FNAME                  add a control vector\n",
      "                                        note: this argument can be repeated to add multiple control vectors\n",
      "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
      "                                        note: this argument can be repeated to add multiple scaled control\n",
      "                                        vectors\n",
      "--control-vector-layer-range START END\n",
      "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
      "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
      "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
      "                                        (env: LLAMA_ARG_MODEL)\n",
      "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
      "                                        (env: LLAMA_ARG_MODEL_URL)\n",
      "-hfr,  --hf-repo REPO                   Hugging Face model repository (default: unused)\n",
      "                                        (env: LLAMA_ARG_HF_REPO)\n",
      "-hff,  --hf-file FILE                   Hugging Face model file (default: unused)\n",
      "                                        (env: LLAMA_ARG_HF_FILE)\n",
      "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
      "                                        variable)\n",
      "                                        (env: HF_TOKEN)\n",
      "-ld,   --logdir LOGDIR                  path under which to save YAML logs (no logging if unset)\n",
      "--log-disable                           Log disable\n",
      "--log-file FNAME                        Log to file\n",
      "--log-colors                            Enable colored logging\n",
      "                                        (env: LLAMA_LOG_COLORS)\n",
      "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
      "                                        debugging)\n",
      "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
      "                                        ignored.\n",
      "                                        (env: LLAMA_LOG_VERBOSITY)\n",
      "--log-prefix                            Enable prefx in log messages\n",
      "                                        (env: LLAMA_LOG_PREFIX)\n",
      "--log-timestamps                        Enable timestamps in log messages\n",
      "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
      "\n",
      "\n",
      "----- sampling params -----\n",
      "\n",
      "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
      "                                        ';'\n",
      "                                        (default: top_k;tfs_z;typ_p;top_p;min_p;temperature)\n",
      "-s,    --seed SEED                      RNG seed (default: 4294967295, use random seed for 4294967295)\n",
      "--sampling-seq SEQUENCE                 simplified sequence for samplers that will be used (default: kfypmt)\n",
      "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
      "                                        --logit-bias EOS-inf)\n",
      "--penalize-nl                           penalize newline tokens (default: false)\n",
      "--temp N                                temperature (default: 0.8)\n",
      "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
      "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
      "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
      "--tfs N                                 tail free sampling, parameter z (default: 1.0, 1.0 = disabled)\n",
      "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
      "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
      "                                        = ctx_size)\n",
      "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
      "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
      "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
      "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
      "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
      "--mirostat N                            use Mirostat sampling.\n",
      "                                        Top K, Nucleus, Tail Free and Locally Typical samplers are ignored if\n",
      "                                        used.\n",
      "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
      "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
      "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
      "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
      "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
      "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
      "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
      "                                        dir) (default: '')\n",
      "--grammar-file FNAME                    file to read grammar from\n",
      "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
      "                                        `{}` for any JSON object\n",
      "                                        For schemas w/ external $refs, use --grammar +\n",
      "                                        example/json_schema_to_grammar.py instead\n",
      "\n",
      "\n",
      "----- example-specific params -----\n",
      "\n",
      "--no-display-prompt                     don't print prompt at generation (default: false)\n",
      "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
      "                                        (default: false)\n",
      "--no-context-shift                      disables context shift on inifinite text generation (default:\n",
      "                                        disabled)\n",
      "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
      "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
      "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
      "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
      "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
      "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
      "-sp,   --special                        special tokens output enabled (default: false)\n",
      "-cnv,  --conversation                   run in conversation mode:\n",
      "                                        - does not print special tokens and suffix/prefix\n",
      "                                        - interactive mode is also enabled\n",
      "                                        (default: false)\n",
      "-i,    --interactive                    run in interactive mode (default: false)\n",
      "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
      "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
      "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
      "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
      "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
      "--no-warmup                             skip warming up the model with an empty run\n",
      "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
      "                                        metadata)\n",
      "                                        if suffix/prefix are specified, template will be disabled\n",
      "                                        only commonly used templates are accepted:\n",
      "                                        https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template\n",
      "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
      "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
      "                                        consoles\n",
      "\n",
      "example usage:\n",
      "\n",
      "  text generation:     llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128\n",
      "\n",
      "  chat (conversation): llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "llama-cli --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebe8fd58-b9b7-4f1c-b446-0bf3515d8115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from ../models/ggufs/Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: control-looking token: '<|end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: control-looking token: '<|endoftext|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special tokens cache size = 67\n",
      "llm_load_vocab: token to piece cache size = 0.1690 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_swa            = 2047\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 2.23 GiB (5.01 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_print_meta: EOG token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 32007 '<|end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    52.84 MiB\n",
      "llm_load_tensors:      Metal buffer size =  2228.83 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  1536.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   300.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    14.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 3979498283\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 4096, n_batch = 2048, n_predict = 400, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Building a website can be done in 10 simple steps:\n",
      "Step 1: Identify your website's purpose, target audience, and goals.\n",
      "Step 2: Choose a domain name and hosting provider.\n",
      "Step 3: Select a website builder or content management system.\n",
      "Step 4: Plan your website's layout and structure.\n",
      "Step 5: Create high-quality, relevant content.\n",
      "Step 6: Optimize your website for SEO (Search Engine Optimization).\n",
      "Step 7: Implement a user-friendly navigation system.\n",
      "Step 8: Add interactive elements and visuals to engage visitors.\n",
      "Step 9: Ensure mobile responsiveness and compatibility.\n",
      "Step 10: Promote your website through social media, email marketing, and other channels.\n",
      "\n",
      "Here's a more detailed breakdown of these steps:\n",
      "\n",
      "Step 1: Identify your website's purpose, target audience, and goals.\n",
      "Before you start building your website, you need to have a clear understanding of its purpose. Is it to sell products, offer services, provide information, or showcase your portfolio? Identifying your target audience helps you tailor your content and design to meet their needs. Setting goals, like increasing sales or generating leads, will help you measure your website's success.\n",
      "\n",
      "Step 2: Choose a domain name and hosting provider.\n",
      "Your domain name is your website's address, so choose one that's easy to remember, relevant to your business or brand, and available. Once you've selected your domain name, you'll need to sign up for a hosting provider, which will give you space on a server to store your website's files.\n",
      "\n",
      "Step 3: Select a website builder or content management system (CMS).\n",
      "There are two main types of tools to help you build your website: website builders and CMS. A website builder is a user-friendly, drag-\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      10.63 ms /   419 runs   (    0.03 ms per token, 39413.04 tokens per second)\n",
      "llama_perf_context_print:        load time =    5215.56 ms\n",
      "llama_perf_context_print: prompt eval time =     141.68 ms /    19 tokens (    7.46 ms per token,   134.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13769.09 ms /   399 runs   (   34.51 ms per token,    28.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   13928.45 ms /   418 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli --model $1 \\\n",
    "          --n-predict 400 \\\n",
    "          --escape \\\n",
    "          --prompt \"Building a website can be done in 10 simple steps:\\nStep 1:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6675560-2fa4-4528-b1d4-37404dbf2688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f51b2bb-829a-4fad-b63c-9c1a97d775b6",
   "metadata": {},
   "source": [
    "**Supported models:**\n",
    "\n",
    "Typically finetunes of the base models below are supported as well.\n",
    "\n",
    "- [X] LLaMA 🦙\n",
    "- [x] LLaMA 2 🦙🦙\n",
    "- [x] LLaMA 3 🦙🦙🦙\n",
    "- [X] [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n",
    "- [x] [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)\n",
    "- [x] [DBRX](https://huggingface.co/databricks/dbrx-instruct)\n",
    "- [X] [Falcon](https://huggingface.co/models?search=tiiuae/falcon)\n",
    "- [X] [Chinese LLaMA / Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) and [Chinese LLaMA-2 / Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)\n",
    "- [X] [Vigogne (French)](https://github.com/bofenghuang/vigogne)\n",
    "- [X] [BERT](https://github.com/ggerganov/llama.cpp/pull/5423)\n",
    "- [X] [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)\n",
    "- [X] [Baichuan 1 & 2](https://huggingface.co/models?search=baichuan-inc/Baichuan) + [derivations](https://huggingface.co/hiyouga/baichuan-7b-sft)\n",
    "- [X] [Aquila 1 & 2](https://huggingface.co/models?search=BAAI/Aquila)\n",
    "- [X] [Starcoder models](https://github.com/ggerganov/llama.cpp/pull/3187)\n",
    "- [X] [Refact](https://huggingface.co/smallcloudai/Refact-1_6B-fim)\n",
    "- [X] [MPT](https://github.com/ggerganov/llama.cpp/pull/3417)\n",
    "- [X] [Bloom](https://github.com/ggerganov/llama.cpp/pull/3553)\n",
    "- [x] [Yi models](https://huggingface.co/models?search=01-ai/Yi)\n",
    "- [X] [StableLM models](https://huggingface.co/stabilityai)\n",
    "- [x] [Deepseek models](https://huggingface.co/models?search=deepseek-ai/deepseek)\n",
    "- [x] [Qwen models](https://huggingface.co/models?search=Qwen/Qwen)\n",
    "- [x] [PLaMo-13B](https://github.com/ggerganov/llama.cpp/pull/3557)\n",
    "- [x] [Phi models](https://huggingface.co/models?search=microsoft/phi)\n",
    "- [x] [GPT-2](https://huggingface.co/gpt2)\n",
    "- [x] [Orion 14B](https://github.com/ggerganov/llama.cpp/pull/5118)\n",
    "- [x] [InternLM2](https://huggingface.co/models?search=internlm2)\n",
    "- [x] [CodeShell](https://github.com/WisdomShell/codeshell)\n",
    "- [x] [Gemma](https://ai.google.dev/gemma)\n",
    "- [x] [Mamba](https://github.com/state-spaces/mamba)\n",
    "- [x] [Grok-1](https://huggingface.co/keyfan/grok-1-hf)\n",
    "- [x] [Xverse](https://huggingface.co/models?search=xverse)\n",
    "- [x] [Command-R models](https://huggingface.co/models?search=CohereForAI/c4ai-command-r)\n",
    "- [x] [SEA-LION](https://huggingface.co/models?search=sea-lion)\n",
    "- [x] [GritLM-7B](https://huggingface.co/GritLM/GritLM-7B) + [GritLM-8x7B](https://huggingface.co/GritLM/GritLM-8x7B)\n",
    "- [x] [OLMo](https://allenai.org/olmo)\n",
    "- [x] [OLMoE](https://huggingface.co/allenai/OLMoE-1B-7B-0924)\n",
    "- [x] [Granite models](https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330)\n",
    "- [x] [GPT-NeoX](https://github.com/EleutherAI/gpt-neox) + [Pythia](https://github.com/EleutherAI/pythia)\n",
    "- [x] [Snowflake-Arctic MoE](https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520)\n",
    "- [x] [Smaug](https://huggingface.co/models?search=Smaug)\n",
    "- [x] [Poro 34B](https://huggingface.co/LumiOpen/Poro-34B)\n",
    "- [x] [Bitnet b1.58 models](https://huggingface.co/1bitLLM)\n",
    "- [x] [Flan T5](https://huggingface.co/models?search=flan-t5)\n",
    "- [x] [Open Elm models](https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759ca)\n",
    "- [x] [ChatGLM3-6b](https://huggingface.co/THUDM/chatglm3-6b) + [ChatGLM4-9b](https://huggingface.co/THUDM/glm-4-9b)\n",
    "- [x] [SmolLM](https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966)\n",
    "- [x] [EXAONE-3.0-7.8B-Instruct](https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct)\n",
    "- [x] [FalconMamba Models](https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a)\n",
    "- [x] [Jais](https://huggingface.co/inceptionai/jais-13b-chat)\n",
    "\n",
    "(instructions for supporting more models: [HOWTO-add-model.md](./docs/development/HOWTO-add-model.md))\n",
    "\n",
    "**Multimodal models:**\n",
    "\n",
    "- [x] [LLaVA 1.5 models](https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e), [LLaVA 1.6 models](https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2)\n",
    "- [x] [BakLLaVA](https://huggingface.co/models?search=SkunkworksAI/Bakllava)\n",
    "- [x] [Obsidian](https://huggingface.co/NousResearch/Obsidian-3B-V0.5)\n",
    "- [x] [ShareGPT4V](https://huggingface.co/models?search=Lin-Chen/ShareGPT4V)\n",
    "- [x] [MobileVLM 1.7B/3B models](https://huggingface.co/models?search=mobileVLM)\n",
    "- [x] [Yi-VL](https://huggingface.co/models?search=Yi-VL)\n",
    "- [x] [Mini CPM](https://huggingface.co/models?search=MiniCPM)\n",
    "- [x] [Moondream](https://huggingface.co/vikhyatk/moondream2)\n",
    "- [x] [Bunny](https://github.com/BAAI-DCAI/Bunny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2b591-bf42-4fdd-a7f4-3a1ba6c59d9c",
   "metadata": {},
   "source": [
    "**Bindings:**\n",
    "\n",
    "- Python: [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python)\n",
    "- Go: [go-skynet/go-llama.cpp](https://github.com/go-skynet/go-llama.cpp)\n",
    "- Node.js: [withcatai/node-llama-cpp](https://github.com/withcatai/node-llama-cpp)\n",
    "- JS/TS (llama.cpp server client): [lgrammel/modelfusion](https://modelfusion.dev/integration/model-provider/llamacpp)\n",
    "- JS/TS (Programmable Prompt Engine CLI): [offline-ai/cli](https://github.com/offline-ai/cli)\n",
    "- JavaScript/Wasm (works in browser): [tangledgroup/llama-cpp-wasm](https://github.com/tangledgroup/llama-cpp-wasm)\n",
    "- Typescript/Wasm (nicer API, available on npm): [ngxson/wllama](https://github.com/ngxson/wllama)\n",
    "- Ruby: [yoshoku/llama_cpp.rb](https://github.com/yoshoku/llama_cpp.rb)\n",
    "- Rust (more features): [edgenai/llama_cpp-rs](https://github.com/edgenai/llama_cpp-rs)\n",
    "- Rust (nicer API): [mdrokz/rust-llama.cpp](https://github.com/mdrokz/rust-llama.cpp)\n",
    "- Rust (more direct bindings): [utilityai/llama-cpp-rs](https://github.com/utilityai/llama-cpp-rs)\n",
    "- C#/.NET: [SciSharp/LLamaSharp](https://github.com/SciSharp/LLamaSharp)\n",
    "- Scala 3: [donderom/llm4s](https://github.com/donderom/llm4s)\n",
    "- Clojure: [phronmophobic/llama.clj](https://github.com/phronmophobic/llama.clj)\n",
    "- React Native: [mybigday/llama.rn](https://github.com/mybigday/llama.rn)\n",
    "- Java: [kherud/java-llama.cpp](https://github.com/kherud/java-llama.cpp)\n",
    "- Zig: [deins/llama.cpp.zig](https://github.com/Deins/llama.cpp.zig)\n",
    "- Flutter/Dart: [netdur/llama_cpp_dart](https://github.com/netdur/llama_cpp_dart)\n",
    "- PHP (API bindings and features built on top of llama.cpp): [distantmagic/resonance](https://github.com/distantmagic/resonance) [(more info)](https://github.com/ggerganov/llama.cpp/pull/6326)\n",
    "- Guile Scheme: [guile_llama_cpp](https://savannah.nongnu.org/projects/guile-llama-cpp)\n",
    "\n",
    "**UI:**\n",
    "\n",
    "Unless otherwise noted these projects are open-source with permissive licensing:\n",
    "\n",
    "- [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT)\n",
    "- [iohub/collama](https://github.com/iohub/coLLaMA)\n",
    "- [janhq/jan](https://github.com/janhq/jan) (AGPL)\n",
    "- [nat/openplayground](https://github.com/nat/openplayground)\n",
    "- [Faraday](https://faraday.dev/) (proprietary)\n",
    "- [LMStudio](https://lmstudio.ai/) (proprietary)\n",
    "- [Layla](https://play.google.com/store/apps/details?id=com.laylalite) (proprietary)\n",
    "- [ramalama](https://github.com/containers/ramalama) (MIT)\n",
    "- [LocalAI](https://github.com/mudler/LocalAI) (MIT)\n",
    "- [LostRuins/koboldcpp](https://github.com/LostRuins/koboldcpp) (AGPL)\n",
    "- [Mozilla-Ocho/llamafile](https://github.com/Mozilla-Ocho/llamafile)\n",
    "- [nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all)\n",
    "- [ollama/ollama](https://github.com/ollama/ollama)\n",
    "- [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) (AGPL)\n",
    "- [psugihara/FreeChat](https://github.com/psugihara/FreeChat)\n",
    "- [cztomsik/ava](https://github.com/cztomsik/ava) (MIT)\n",
    "- [ptsochantaris/emeltal](https://github.com/ptsochantaris/emeltal)\n",
    "- [pythops/tenere](https://github.com/pythops/tenere) (AGPL)\n",
    "- [RAGNA Desktop](https://ragna.app/) (proprietary)\n",
    "- [RecurseChat](https://recurse.chat/) (proprietary)\n",
    "- [semperai/amica](https://github.com/semperai/amica)\n",
    "- [withcatai/catai](https://github.com/withcatai/catai)\n",
    "- [Mobile-Artificial-Intelligence/maid](https://github.com/Mobile-Artificial-Intelligence/maid) (MIT)\n",
    "- [Msty](https://msty.app) (proprietary)\n",
    "- [LLMFarm](https://github.com/guinmoon/LLMFarm?tab=readme-ov-file) (MIT)\n",
    "- [KanTV](https://github.com/zhouwg/kantv?tab=readme-ov-file)(Apachev2.0 or later)\n",
    "- [Dot](https://github.com/alexpinel/Dot) (GPL)\n",
    "- [MindMac](https://mindmac.app) (proprietary)\n",
    "- [KodiBot](https://github.com/firatkiral/kodibot) (GPL)\n",
    "- [eva](https://github.com/ylsdamxssjxxdd/eva) (MIT)\n",
    "- [AI Sublime Text plugin](https://github.com/yaroslavyaroslav/OpenAI-sublime-text) (MIT)\n",
    "- [AIKit](https://github.com/sozercan/aikit) (MIT)\n",
    "- [LARS - The LLM & Advanced Referencing Solution](https://github.com/abgulati/LARS) (AGPL)\n",
    "- [LLMUnity](https://github.com/undreamai/LLMUnity) (MIT)\n",
    "\n",
    "*(to have a project listed here, it should clearly state that it depends on `llama.cpp`)*\n",
    "\n",
    "**Tools:**\n",
    "\n",
    "- [akx/ggify](https://github.com/akx/ggify) – download PyTorch models from HuggingFace Hub and convert them to GGML\n",
    "- [crashr/gppm](https://github.com/crashr/gppm) – launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption\n",
    "- [gpustack/gguf-parser](https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser) - review/check the GGUF file and estimate the memory usage\n",
    "- [Styled Lines](https://marketplace.unity.com/packages/tools/generative-ai/styled-lines-llama-cpp-model-292902) (proprietary licensed, async wrapper of inference part for game development in Unity3d with prebuild Mobile and Web platform wrappers and a model example)\n",
    "\n",
    "**Infrastructure:**\n",
    "\n",
    "- [Paddler](https://github.com/distantmagic/paddler) - Stateful load balancer custom-tailored for llama.cpp\n",
    "- [GPUStack](https://github.com/gpustack/gpustack) - Manage GPU clusters for running LLMs\n",
    "\n",
    "**Games:**\n",
    "- [Lucy's Labyrinth](https://github.com/MorganRO8/Lucys_Labyrinth) - A simple maze game where agents controlled by an AI model will try to trick you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927dc9b5-7b30-496e-ac69-9e7befd98317",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "Here are the end-to-end binary build and model conversion steps for most supported models.\n",
    "\n",
    "### Basic usage\n",
    "\n",
    "Firstly, you need to get the binary. There are different methods that you can follow:\n",
    "- Method 1: Clone this repository and build locally, see [how to build](./docs/build.md)\n",
    "- Method 2: If you are using MacOS or Linux, you can install llama.cpp via [brew, flox or nix](./docs/install.md)\n",
    "- Method 3: Use a Docker image, see [documentation for Docker](./docs/docker.md)\n",
    "- Method 4: Download pre-built binary from [releases](https://github.com/ggerganov/llama.cpp/releases)\n",
    "\n",
    "You can run a basic completion using this command:\n",
    "\n",
    "```bash\n",
    "llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128\n",
    "\n",
    "# Output:\n",
    "# I believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don't align with societal expectations. I think that's what I love about yoga – it's not just a physical practice, but a spiritual one too. It's about connecting with yourself, listening to your inner voice, and honoring your own unique journey.\n",
    "```\n",
    "\n",
    "See [this page](./examples/main/README.md) for a full list of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30843a-dd2e-4684-845e-c5e5d3523eeb",
   "metadata": {},
   "source": [
    "### Conversation mode\n",
    "\n",
    "If you want a more ChatGPT-like experience, you can run in conversation mode by passing `-cnv` as a parameter:\n",
    "\n",
    "```bash\n",
    "llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv\n",
    "\n",
    "# Output:\n",
    "# > hi, who are you?\n",
    "# Hi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist and provide information to users like you. I'm here to help answer your questions, provide guidance, and offer support on a wide range of topics. I'm a friendly and knowledgeable AI, and I'm always happy to help with anything you need. What's on your mind, and how can I assist you today?\n",
    "#\n",
    "# > what is 1+1?\n",
    "# Easy peasy! The answer to 1+1 is... 2!\n",
    "```\n",
    "\n",
    "By default, the chat template will be taken from the input model. If you want to use another chat template, pass `--chat-template NAME` as a parameter. See the list of [supported templates](https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template)\n",
    "\n",
    "```bash\n",
    "./llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv --chat-template chatml\n",
    "```\n",
    "\n",
    "You can also use your own template via in-prefix, in-suffix and reverse-prompt parameters:\n",
    "\n",
    "```bash\n",
    "./llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv --in-prefix 'User: ' --reverse-prompt 'User:'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
