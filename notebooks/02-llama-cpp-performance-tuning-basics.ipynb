{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6256957-6043-4bba-8f0c-92b199790d47",
   "metadata": {},
   "source": [
    "# Performance Tuning and Memory Options\n",
    "\n",
    "These options help improve the performance and memory usage of the LLaMA models. By adjusting these settings, you can fine-tune the model's behavior to better suit your system's capabilities and achieve optimal performance for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a048cd5-2e6c-4c57-a667-ad78942fd660",
   "metadata": {},
   "source": [
    "## Number of Threads\n",
    "\n",
    "-   `-t N, --threads N`: Set the number of threads to use during generation. For optimal performance, it is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Using the correct number of threads can greatly improve performance.\n",
    "-   `-tb N, --threads-batch N`: Set the number of threads to use during batch and prompt processing. In some systems, it is beneficial to use a higher number of threads during batch processing than during generation. If not specified, the number of threads used for batch processing will be the same as the number of threads used for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d9eb2-5c21-420f-9a6c-373f01276023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350ba86-1aa3-40da-a7de-2902a29c4851",
   "metadata": {},
   "source": [
    "## Mlock\n",
    "\n",
    "-   `--mlock`: Lock the model in memory, preventing it from being swapped out when memory-mapped. This can improve performance but trades away some of the advantages of memory-mapping by requiring more RAM to run and potentially slowing down load times as the model loads into RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c67d2-1bdf-4a0f-afe4-6ec6b4ad966e",
   "metadata": {},
   "source": [
    "## No Memory Mapping\n",
    "\n",
    "-   `--no-mmap`: Do not memory-map the model. By default, models are mapped into memory, which allows the system to load only the necessary parts of the model as needed. However, if the model is larger than your total amount of RAM or if your system is low on available memory, using mmap might increase the risk of pageouts, negatively impacting performance. Disabling mmap results in slower load times but may reduce pageouts if you're not using `--mlock`. Note that if the model is larger than the total amount of RAM, turning off mmap would prevent the model from loading at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afdf19c-7ab4-401c-aa0c-30390ca7d80b",
   "metadata": {},
   "source": [
    "## NUMA support\n",
    "\n",
    "-   `--numa distribute`: Pin an equal proportion of the threads to the cores on each NUMA node. This will spread the load amongst all cores on the system, utilitizing all memory channels at the expense of potentially requiring memory to travel over the slow links between nodes.\n",
    "-   `--numa isolate`: Pin all threads to the NUMA node that the program starts on. This limits the number of cores and amount of memory that can be used, but guarantees all memory access remains local to the NUMA node.\n",
    "-   `--numa numactl`: Pin threads to the CPUMAP that is passed to the program by starting it with the numactl utility. This is the most flexible mode, and allow arbitrary core usage patterns, for example a map that uses all the cores on one NUMA nodes, and just enough cores on a second node to saturate the inter-node memory bus.\n",
    "\n",
    " These flags attempt optimizations that help on some systems with non-uniform memory access. This currently consists of one of the above strategies, and disabling prefetch and readahead for mmap. The latter causes mapped pages to be faulted in on first access instead of all at once, and in combination with pinning threads to NUMA nodes, more of the pages end up on the NUMA node where they are used. Note that if the model is already in the system page cache, for example because of a previous run without this option, this will have little effect unless you drop the page cache first. This can be done by rebooting the system or on Linux by writing '3' to '/proc/sys/vm/drop_caches' as root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e29fe2-d43b-4289-b1e2-a10444a7f2c5",
   "metadata": {},
   "source": [
    "## Batch Size\n",
    "\n",
    "-   `-b N, --batch-size N`: Set the batch size for prompt processing (default: `2048`). This large batch size benefits users who have BLAS installed and enabled it during the build. If you don't have BLAS enabled (\"BLAS=0\"), you can use a smaller number, such as 8, to see the prompt progress as it's evaluated in some situations.\n",
    "\n",
    "- `-ub N`, `--ubatch-size N`: physical maximum batch size. This is for pipeline parallelization. Default: `512`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3474811e-f8b7-425c-a6e2-e4b98378d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9386e12-59d5-41f2-b8c2-6f5152e7d9d4",
   "metadata": {},
   "source": [
    "## Prompt Caching\n",
    "\n",
    "-   `--prompt-cache FNAME`: Specify a file to cache the model state after the initial prompt. This can significantly speed up the startup time when you're using longer prompts. The file is created during the first run and is reused and updated in subsequent runs. **Note**: Restoring a cached prompt does not imply restoring the exact state of the session at the point it was saved. So even when specifying a specific seed, you are not guaranteed to get the same sequence of tokens as the original generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432db73-fe5b-43a2-b8e3-5732e3b05314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b38173-24ac-4d49-94cf-5e54e2cf7724",
   "metadata": {},
   "source": [
    "## Grammars & JSON schemas\n",
    "\n",
    "-   `--grammar GRAMMAR`, `--grammar-file FILE`: Specify a grammar (defined inline or in a file) to constrain model output to a specific format. For example, you could force the model to output JSON or to speak only in emojis. See the [GBNF guide](../../grammars/README.md) for details on the syntax.\n",
    "\n",
    "-   `--json-schema SCHEMA`: Specify a [JSON schema](https://json-schema.org/) to constrain model output to (e.g. `{}` for any JSON object, or `{\"items\": {\"type\": \"string\", \"minLength\": 10, \"maxLength\": 100}, \"minItems\": 10}` for a JSON array of strings with size constraints). If a schema uses external `$ref`s, you should use `--grammar \"$( python examples/json_schema_to_grammar.py myschema.json )\"` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b88e2e-57f8-4d5b-b752-8671cabed28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bfe1cd-64c8-465c-a08c-63c484d08d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
