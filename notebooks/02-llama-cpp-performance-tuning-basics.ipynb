{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6256957-6043-4bba-8f0c-92b199790d47",
   "metadata": {},
   "source": [
    "# Performance Tuning\n",
    "\n",
    "By adjusting these settings, you can fine-tune the model's behavior to better suit your system's capabilities and achieve optimal performance for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c85a4-4edd-476d-96d4-a1d3df3210c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"../models/gemma-1.1-7b-it.Q4_K_M.gguf\"\n",
    "PROMPT_FILE = \"../prompts/engaging-twitter-thread.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87339d-59cb-4b67-b1a0-dcaca152f898",
   "metadata": {},
   "source": [
    "## CPU usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcae49-945d-4ebb-b221-31be7a092215",
   "metadata": {},
   "source": [
    "### Controling the number of threads\n",
    "\n",
    "There are two flags that you can use to control the number of CPU threads used by LLaMA C++.\n",
    "\n",
    "-   `--threads N`: Set the number of threads to use during generation. For optimal performance, it is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Using the correct number of threads can greatly improve performance.\n",
    "-   `--threads-batch N`: Set the number of threads to use during batch and prompt processing. In some systems, it is beneficial to use a higher number of threads during batch processing than during generation. If not specified, the number of threads used for batch processing will be the same as the number of threads used for generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3705f7c-8ebb-43fa-aef2-f8008c3356ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --prompt \"Why is the sky blue?\" \\\n",
    "    --seed 42 \\\n",
    "    --threads -1 \\\n",
    "    --threads-batch -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d9eb2-5c21-420f-9a6c-373f01276023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "# this could be physical or logical depending on OS? \n",
    "print(mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15a80c-9425-4c93-9bfe-9e6d69db9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --prompt \"Why is the sky blue?\" \\\n",
    "    --seed 42\n",
    "    --threads 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afdf19c-7ab4-401c-aa0c-30390ca7d80b",
   "metadata": {},
   "source": [
    "### NUMA support\n",
    "\n",
    "If you are running LLaMA C++ on a [Non-uniform Memory Access NUMA](https://en.wikipedia.org/wiki/Non-uniform_memory_access) system then the following flags can help you tune performance.\n",
    "\n",
    "-   `--numa distribute`: Pin an equal proportion of the threads to the cores on each NUMA node. This will spread the load amongst all cores on the system, utilitizing all memory channels at the expense of potentially requiring memory to travel over the slow links between nodes.\n",
    "-   `--numa isolate`: Pin all threads to the NUMA node that the program starts on. This limits the number of cores and amount of memory that can be used, but guarantees all memory access remains local to the NUMA node.\n",
    "-   `--numa numactl`: Pin threads to the CPUMAP that is passed to the program by starting it with the numactl utility. This is the most flexible mode, and allow arbitrary core usage patterns, for example a map that uses all the cores on one NUMA nodes, and just enough cores on a second node to saturate the inter-node memory bus.\n",
    "\n",
    "These flags attempt optimizations that help on some systems with non-uniform memory access. This currently consists of one of the above strategies, and disabling prefetch and readahead for mmap. The latter causes mapped pages to be faulted in on first access instead of all at once, and in combination with pinning threads to NUMA nodes, more of the pages end up on the NUMA node where they are used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1822dcc7-c40c-48a6-a47f-71b815ae4a1f",
   "metadata": {},
   "source": [
    "## GPU usage\n",
    "\n",
    "There are a couple of flags that can be used to tune performance when using multiple GPUs.\n",
    "\n",
    "-   `--main-gpu i`: When using multiple GPUs this option controls which GPU is used for small tensors for which the overhead of splitting the computation across all GPUs is not worthwhile. The GPU in question will use slightly more VRAM to store a scratch buffer for temporary results. By default GPU 0 is used.\n",
    "-   `--tensor-split SPLIT`: When using multiple GPUs this option controls how large tensors should be split across all GPUs. `SPLIT` is a comma-separated list of non-negative values that assigns the proportion of data that each GPU should get in order. For example, \"3,2\" will assign 60% of the data to GPU 0 and 40% to GPU 1. By default the data is split in proportion to VRAM but this may not be optimal for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d16a3-3483-40c6-ab8d-f2b9420253d3",
   "metadata": {},
   "source": [
    "## Memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350ba86-1aa3-40da-a7de-2902a29c4851",
   "metadata": {},
   "source": [
    "### Mlock\n",
    "\n",
    "-   `--mlock`: Lock the model in memory, preventing it from being swapped out when memory-mapped. This can improve performance but trades away some of the advantages of memory-mapping by requiring more RAM to run and potentially slowing down load times as the model loads into RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c67d2-1bdf-4a0f-afe4-6ec6b4ad966e",
   "metadata": {},
   "source": [
    "### No Memory Mapping\n",
    "\n",
    "-   `--no-mmap`: Do not memory-map the model. By default, models are mapped into memory, which allows the system to load only the necessary parts of the model as needed. However, if the model is larger than your total amount of RAM or if your system is low on available memory, using mmap might increase the risk of pageouts, negatively impacting performance. Disabling mmap results in slower load times but may reduce pageouts if you're not using `--mlock`. Note that if the model is larger than the total amount of RAM, turning off mmap would prevent the model from loading at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6836f-fa45-4a0b-9442-911fd40cefe3",
   "metadata": {},
   "source": [
    "## Other tips and tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69470bd8-cfbf-4032-bdf7-8b99485a00a6",
   "metadata": {},
   "source": [
    "### Prompt Caching\n",
    "\n",
    "Specify a file to cache the model state after the initial prompt using `--prompt-cache FNAME`.  This can significantly speed up the startup time when you're using longer prompts. The file is created during the first run and is reused and updated in subsequent runs. \n",
    "\n",
    "Restoring a cached prompt does not imply restoring the exact state of the session at the point it was saved. So even when specifying a specific seed, you are not guaranteed to get the same sequence of tokens as the original generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e29fe2-d43b-4289-b1e2-a10444a7f2c5",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "\n",
    "-   `-b N, --batch-size N`: Set the batch size for prompt processing (default: `2048`). This large batch size benefits users who have BLAS installed and enabled it during the build. If you don't have BLAS enabled (\"BLAS=0\"), you can use a smaller number, such as 8, to see the prompt progress as it's evaluated in some situations.\n",
    "\n",
    "- `-ub N`, `--ubatch-size N`: physical maximum batch size. This is for pipeline parallelization. Default: `512`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14999956-fed4-4faa-bd4b-fca742127176",
   "metadata": {},
   "source": [
    "### Grammars & JSON schemas\n",
    "\n",
    "-   `--grammar GRAMMAR`, `--grammar-file FILE`: Specify a grammar (defined inline or in a file) to constrain model output to a specific format. For example, you could force the model to output JSON or to speak only in emojis. See the [GBNF guide](../../grammars/README.md) for details on the syntax.\n",
    "\n",
    "-   `--json-schema SCHEMA`: Specify a [JSON schema](https://json-schema.org/) to constrain model output to (e.g. `{}` for any JSON object, or `{\"items\": {\"type\": \"string\", \"minLength\": 10, \"maxLength\": 100}, \"minItems\": 10}` for a JSON array of strings with size constraints). If a schema uses external `$ref`s, you should use `--grammar \"$( python examples/json_schema_to_grammar.py myschema.json )\"` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22aedb-cda4-41a0-a135-0961034a4486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
