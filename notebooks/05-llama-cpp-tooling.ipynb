{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0e1ba6-93d5-40f6-a5ac-d9a4848e8b20",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "### Prepare and Quantize\n",
    "\n",
    "> [!NOTE]\n",
    "> You can use the [GGUF-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) space on Hugging Face to quantise your model weights without any setup too. It is synced from `llama.cpp` main every 6 hours.\n",
    "\n",
    "To obtain the official LLaMA 2 weights please see the <a href=\"#obtaining-and-using-the-facebook-llama-2-model\">Obtaining and using the Facebook LLaMA 2 model</a> section. There is also a large selection of pre-quantized `gguf` models available on Hugging Face.\n",
    "\n",
    "Note: `convert.py` has been moved to `examples/convert_legacy_llama.py` and shouldn't be used for anything other than `Llama/Llama2/Mistral` models and their derivatives.\n",
    "It does not support LLaMA 3, you can use `convert_hf_to_gguf.py` with LLaMA 3 downloaded from Hugging Face.\n",
    "\n",
    "To learn more about quantizing model, [read this documentation](./examples/quantize/README.md)\n",
    "\n",
    "### Perplexity (measuring model quality)\n",
    "\n",
    "You can use the `perplexity` example to measure perplexity over a given prompt (lower perplexity is better).\n",
    "For more information, see [https://huggingface.co/docs/transformers/perplexity](https://huggingface.co/docs/transformers/perplexity).\n",
    "\n",
    "To learn more how to measure perplexity using llama.cpp, [read this documentation](./examples/perplexity/README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
