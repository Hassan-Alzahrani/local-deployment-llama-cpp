{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f783e6-58ce-45d9-ab77-1b84807c5baf",
   "metadata": {},
   "source": [
    "# Getting started with LLaMA C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df8c16-7f4f-4e93-aa19-fdcb0d8d1b3f",
   "metadata": {},
   "source": [
    "## 1. Quick Start\n",
    "\n",
    "Open a terminal and run the following command to check that the `llama-cli` binary is available.\n",
    "\n",
    "```bash\n",
    "which llama-cli\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e622f-b34b-4404-8351-9705ffe21e59",
   "metadata": {},
   "source": [
    "### Input prompt (One-and-done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139a94c-9dc9-4ee7-9a24-6ec2b0858452",
   "metadata": {},
   "source": [
    "#### Manually downloading a model from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f69b90-573c-416b-8d78-cc8fb85ddd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1134  100  1134    0     0   3127      0 --:--:-- --:--:-- --:--:--  3132\n",
      "100 5082M  100 5082M    0     0  10.1M      0  0:08:21  0:08:21 --:--:-- 11.1M\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_URL=https://huggingface.co/ggml-org/gemma-1.1-7b-it-Q4_K_M-GGUF/resolve/main/gemma-1.1-7b-it.Q4_K_M.gguf\n",
    "curl --location --output ../models/gemma-1.1-7b-it.Q4_K_M.gguf $MODEL_URL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e4ed5-7458-427a-bf79-838cecd6423f",
   "metadata": {},
   "source": [
    "After downloading the model file we can just pass the path to the model file as a command line argument.\n",
    "\n",
    "-   `-m FNAME, --model FNAME`: Specify the path to the LLaMA model file.\n",
    "\n",
    "```bash\n",
    "MODEL=./models/gemma-1.1-7b-it.Q4_K_M.gguf\n",
    "llama-cli --model $MODEL --prompt \"Once upon a time\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9d622-b0dc-4500-9bb6-559d9e24b577",
   "metadata": {},
   "source": [
    "#### Downloading a model directly from a URL\n",
    "\n",
    "The command below makes use of the following options.\n",
    "\n",
    "-   `-mu MODEL_URL --model-url MODEL_URL`: Specify a remote http url to download the file.\n",
    "\n",
    "```bash\n",
    "MODEL_URL=https://huggingface.co/ggml-org/gemma-1.1-7b-it-Q4_K_M-GGUF/resolve/main/gemma-1.1-7b-it.Q4_K_M.gguf\n",
    "llama-cli --model-url \"$MODEL_URL\" --prompt \"Once upon a time\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717904a8-6348-4a60-ae72-4afcddd87dde",
   "metadata": {},
   "source": [
    "### Conversation mode (Allow for continuous interaction with the model)\n",
    "\n",
    "The command below makes use of the following options.\n",
    "\n",
    "- `-cnv,  --conversation`: run in conversation mode:\n",
    "  - does not print special tokens and suffix/prefix\n",
    "  - interactive mode is also enabled.\n",
    "- `--chat-template JINJA_TEMPLATE`: Set custom jinja chat template (default: template taken from model's metadata) if suffix/prefix are specified, see the [LLaMA C++ documentation](https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template) for the current list of accepted chat templates.\n",
    "\n",
    "```bash\n",
    "CHAT_TEMPLATE=gemma\n",
    "llama-cli --model $MODEL --conversation --chat-template $CHAT_TEMPLATE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6042b2-0c3d-4831-ad3a-3711fdcae1c5",
   "metadata": {},
   "source": [
    "## 2. Input Prompts\n",
    "\n",
    "The `llama-cli` program provides several ways to interact with the LLaMA models using input prompts:\n",
    "\n",
    "-   `--prompt PROMPT`: Provide a prompt directly as a command-line option.\n",
    "-   `--file FNAME`: Provide a file containing a prompt or multiple prompts.\n",
    "-   `--interactive-first`: Run the program in interactive mode and wait for input right away. (More on this below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16316d6-88a3-484c-a431-aad649c4af21",
   "metadata": {},
   "source": [
    "### `--prompt` example\n",
    "\n",
    "```bash\n",
    "llama-cli --model \"$MODEL\" --prompt \"What is the meaning of life?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523dcebb-4cbd-4abc-8b26-c2978e32ce63",
   "metadata": {},
   "source": [
    "### `--file` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a9f43a-a001-4847-b995-77b3294767e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"English\"\n",
    "tone_of_voice = \"Informative\"\n",
    "topic = \"Computer Science\"\n",
    "writing_style = \"Conversational\"\n",
    "\n",
    "prompt_template = f\"\"\"Please ignore all previous instructions. Please respond \\\n",
    "only in the {language} language. You are a Twitter influencer with a large \\\n",
    "following. You have a {tone_of_voice} tone of voice. You have a \\\n",
    "{writing_style} writing style. Do not self reference. Do not explain what you \\\n",
    "are doing. Please create a thread about {topic}. Add emojis to the thread \\\n",
    "when appropriate. The character count for each thread should be between 270 \\\n",
    "to 280 characters. Your content should be casual, informative, and an \\\n",
    "engaging Twitter thread. Please use simple and understandable words. Please \\\n",
    "include statistics, personal experience, and fun facts in the thread. Please \\\n",
    "add relevant hashtags to the post and encourage the readers join the \\\n",
    "conversation.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f85285ed-ddcc-4997-b454-589885e74224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e7ded9-d5ee-47c3-a5b6-cb1f6e29a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../prompts/engaging-twitter-thread.txt\", 'w') as f:\n",
    "    f.write(prompt_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150fc4b0-4b8c-4f80-93dc-e661fa5030fc",
   "metadata": {},
   "source": [
    "```bash\n",
    "llama-cli --model \"$MODEL\" --file ./prompts/engaging-twitter-thread.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b5347-7b20-455c-b5e9-57a9f702d5dd",
   "metadata": {},
   "source": [
    "### `--interactive-first` example\n",
    "\n",
    "```bash\n",
    "llama-cli --model \"$MODEL\" --interactive-first\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fcc6c2-355f-4607-a0ec-265409afac08",
   "metadata": {},
   "source": [
    "## 3. Interaction\n",
    "\n",
    "The `llama-cli` program offers a seamless way to interact with LLaMA models, allowing users to engage in real-time conversations or provide instructions for specific tasks. The interactive mode can be triggered using various options, including `--interactive` and `--interactive-first`.\n",
    "\n",
    "In interactive mode, users can participate in text generation by injecting their input during the process. Users can press `Ctrl+C` at any time to interject and type their input, followed by pressing `Return` to submit it to the LLaMA model. To submit additional lines without finalizing input, users can end the current line with a backslash (`\\`) and continue typing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174e0c6-785b-4d8f-a190-3e5f07f51f5f",
   "metadata": {},
   "source": [
    "### Interaction Options\n",
    "\n",
    "-   `-i, --interactive`: Run the program in interactive mode, allowing users to engage in real-time conversations or provide specific instructions to the model.\n",
    "-   `--interactive-first`: Run the program in interactive mode and immediately wait for user input before starting the text generation.\n",
    "-   `-cnv,  --conversation`:  Run the program in conversation mode (does not print special tokens and suffix/prefix, use default chat template) (default: false)\n",
    "-   `--color`: Enable colorized output to differentiate visually distinguishing between prompts, user input, and generated text.\n",
    "\n",
    "By understanding and utilizing these interaction options, you can create engaging and dynamic experiences with your models, tailoring the text generation process to your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ac85db-e729-4a94-9c41-4807cb6055b1",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Type the following command in the terminal.\n",
    "\n",
    "```bash\n",
    "llama-cli --model \"$MODEL\" --conversation --color\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cff61d-6909-4910-9bcd-a6eec2d90905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "143d6b1e-347c-4809-8991-d1747f3e0017",
   "metadata": {},
   "source": [
    "### Reverse Prompts\n",
    "\n",
    "Reverse prompts are a powerful way to create a chat-like experience with your model by pausing the text generation when specific text strings are encountered using the `--reverse-prompt` option.\n",
    "\n",
    "-   `-r PROMPT, --reverse-prompt PROMPT`: Specify one or multiple reverse prompts to pause text generation and switch to interactive mode. For example, `-r \"User:\"` can be used to jump back into the conversation whenever it's the user's turn to speak. This helps create a more interactive and conversational experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896fc2a-56c4-4270-adab-94e2c99e6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036b8d7-294a-4324-bce7-acac57565a2f",
   "metadata": {},
   "source": [
    "### In-Prefix\n",
    "\n",
    "The `--in-prefix` flag is used to add a prefix to your input, primarily, this is used to insert a space after the reverse prompt. Here's an example of how to use the `--in-prefix` flag in conjunction with the `--reverse-prompt` flag:\n",
    "\n",
    "```sh\n",
    "llama-cli -r \"User:\" --in-prefix \" \"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e481ba-c57b-4da9-8106-1f1e57eaca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb58c5-fae8-4f2a-9e9c-e1a73caca731",
   "metadata": {},
   "source": [
    "### In-Suffix\n",
    "\n",
    "The `--in-suffix` flag is used to add a suffix after your input. This is useful for adding an \"Assistant:\" prompt after the user's input. It's added after the new-line character (`\\n`) that's automatically added to the end of the user's input. Here's an example of how to use the `--in-suffix` flag in conjunction with the `--reverse-prompt` flag:\n",
    "\n",
    "```sh\n",
    "./llama-cli -r \"User:\" --in-prefix \" \" --in-suffix \"Assistant:\"\n",
    "```\n",
    "When --in-prefix or --in-suffix options are enabled the chat template ( --chat-template ) is disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac75716-7119-4bf9-bf6d-b441601b575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7467e-108a-486d-9344-90feb85a7585",
   "metadata": {},
   "source": [
    "### Chat templates\n",
    "\n",
    " `--chat-template JINJA_TEMPLATE`: This option sets a custom jinja chat template. It accepts a string, not a file name.  Default: template taken from model's metadata. Llama.cpp only supports [some pre-defined templates](https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template). These include the following\n",
    "\n",
    "- `llama2`\n",
    "- `llama3`\n",
    "- `gemma`\n",
    "- `monarch`\n",
    "- `chatml`\n",
    "- `orion`\n",
    "- `vicuna`\n",
    "- `vicuna-orca`\n",
    "- `deepseek`\n",
    "- `command-r`\n",
    "- `zephyr`\n",
    "\n",
    "When `--in-prefix` or `--in-suffix` options are enabled the chat template ( `--chat-template` ) is disabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36958da5-1560-49d7-b4db-6eae750f1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482676a-0521-42e4-b664-cdfb603832c8",
   "metadata": {},
   "source": [
    "## 4. Context Management\n",
    "\n",
    "During text generation, models have a limited context size, which means they can only consider a certain number of tokens from the input and generated text. When the context fills up, the model resets internally,  otentially losing some information from the beginning of the conversation or instructions. Context management options help maintain continuity and coherence in these situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c4465-8973-4f7d-ae98-3dc0d00fb947",
   "metadata": {},
   "source": [
    "### Context Size\n",
    "\n",
    "- `-c N, --ctx-size N`: Set the size of the prompt context (default: 0, 0 = loaded from model). The LLaMA models were built with a context of 2048-8192, which will yield the best results on longer input/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de63d677-2ba3-4326-91d2-1a7e49f9d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d951e5d7-9474-427f-bba0-a53475267243",
   "metadata": {},
   "source": [
    "### Extended Context Size\n",
    "\n",
    "Some fine-tuned models have extended the context length by scaling RoPE. For example, if the original pre-trained model has a context length (max sequence length) of 4096 (4k) and the fine-tuned model has 32k. That is a scaling factor of 8, and should work by setting the above `--ctx-size` to 32768 (32k) and `--rope-scale` to 8.\n",
    "\n",
    "-   `--rope-scale N`: Where N is the linear scaling factor used by the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02c95a-f3fd-4cee-bc0b-314de435e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e3780-7b64-4f31-9223-382a0e792754",
   "metadata": {},
   "source": [
    "### Keep Prompt\n",
    "\n",
    "The `--keep` option allows users to retain the original prompt when the model runs out of context, ensuring a connection to the initial instruction or conversation topic is maintained.\n",
    "\n",
    "-   `--keep N`: Specify the number of tokens from the initial prompt to retain when the model resets its internal context. By default, this value is set to 0 (meaning no tokens are kept). Use `-1` to retain all tokens from the initial prompt.\n",
    "\n",
    "By utilizing context management options like `--ctx-size` and `--keep`, you can maintain a more coherent and consistent interaction with the LLaMA models, ensuring that the generated text remains relevant to the original prompt or conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c166bc-9ea9-4c61-8f4a-abf238794a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97500427-9c68-4273-bcab-b0a64ff18a68",
   "metadata": {},
   "source": [
    "## 5. Generation Flags\n",
    "\n",
    "The following options allow you to control the text generation process and fine-tune the diversity, creativity, and quality of the generated text according to your needs. By adjusting these options and experimenting with different combinations of values, you can find the best settings for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e09d7fcc-b392-47b5-b6ab-7eccee6fc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"../models/gemma-1.1-7b-it.Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d0866-b4ff-43a8-94c2-9029c0abf4d4",
   "metadata": {},
   "source": [
    "### Random Number Generator (RNG) Seed\n",
    "\n",
    "-   `-s SEED, --seed SEED`: Set the random number generator (RNG) seed (default: -1, -1 = random seed).\n",
    "\n",
    "The RNG seed is used to initialize the random number generator that influences the text generation process. By setting a specific `--seed` value, you can obtain consistent and reproducible results across multiple runs with the same input and settings. This can be helpful for testing, debugging, or comparing the effects of different options on the generated text to see when they diverge. If the seed is set to a value less than 0, a random seed will be used, which will result in different outputs on each run. The default value is -1 which will choose a random value for `--seed`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a676c8-5d90-425f-a900-b4b0694f38f8",
   "metadata": {},
   "source": [
    "#### Random `--seed` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9352f9d0-3861-4e6c-9b6c-1e3fb58ca8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 3025719086\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following.\u001b[0m You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "## Thread: The Magic of Computer Science ✨💻\n",
      "\n",
      "Ever wondered how apps translate your language to code? Or how websites load in a blink? That's the power of **Computer Science**! 💻📚\n",
      "\n",
      "It's the building block of the digital age, shaping everything from your smartphone to the future of space exploration 🚀. \n",
      "\n",
      "Did you know? 🤯\n",
      "\n",
      "- There are over **2.5 million computer science students** globally! 🌎\n",
      "- The industry is projected to grow by **23%** by 2030, way faster than any other field. 📈\n",
      "\n",
      "But it's not just about coding! 💻\n",
      "\n",
      "Computer science is about problem-solving, creativity, and building elegant solutions to complex challenges. 💪🧠\n",
      "\n",
      "It's about understanding algorithms, data structures, and operating systems - the building blocks of every digital device you use. 💻💻\n",
      "\n",
      "So if you're curious about the digital world, consider exploring the fascinating world of Computer Science! 💻✨ #CSisAwesome #DigitalFuture #TechLife #JoinTheConversation [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      20.24 ms /   366 runs   (    0.06 ms per token, 18085.68 tokens per second)\n",
      "llama_perf_context_print:        load time =    2310.27 ms\n",
      "llama_perf_context_print: prompt eval time =    1131.81 ms /   141 tokens (    8.03 ms per token,   124.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14383.53 ms /   224 runs   (   64.21 ms per token,    15.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   15548.33 ms /   365 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --seed -1 \\\n",
    "    --color \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ecde3-e4ac-4983-a8cf-226bc70eef1d",
   "metadata": {},
   "source": [
    "### Fixed `--seed` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1ed26f2-7f4c-459b-93f6-782f8d4321b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 42\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\u001b[0m\n",
      "\n",
      "**Tweet 1/5**\n",
      "\n",
      "Hey there, code enthusiasts! 💻✨ Ever wondered how apps keep running smoothly or how complex websites load in seconds? The magic behind that is **Computer Science**! 💪 It's the science of designing, building, and maintaining the software that powers our digital world 💻🌎. #CSIsEverywhere #TechChat\n",
      "\n",
      "**Tweet 2/5**\n",
      "\n",
      "Did you know? 🤯 Over 6 million computer science graduates are expected worldwide by 2030! 🤯 The industry is booming with opportunities, with average salaries reaching $114,500 in the US 💰. If you're curious about shaping the future, CS might be the perfect field for you! #FutureOfWork #CSCareers\n",
      "\n",
      "**Tweet 3/5**\n",
      "\n",
      "Beyond just coding, Computer Science is about **solving problems** 💡. It's about applying technology to areas like healthcare, education, transportation, and even space exploration 🚀. Imagine developing AI that helps diagnose diseases or building robots that assist with disaster relief! #CSImpact #Innovation\n",
      "\n",
      "**Tweet 4/5**\n",
      "\n",
      "Here's a fun fact: 💡 The first computer program was written by Ada Lovelace in 1840! 🤯 Her work laid the groundwork for modern-day programming languages like Python and Java. #WomenInCS #AdaLovelace\n",
      "\n",
      "**Tweet 5/5**\n",
      "\n",
      "If you're fascinated by the power of technology and want to make a difference in the world, consider exploring Computer Science! 💻📚 There are countless resources available to get started, from online courses to bootcamps and internships. Join the movement and let's build a future filled with innovative solutions! #CSCommunity #LearnCS #TechForGood [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      33.70 ms /   508 runs   (    0.07 ms per token, 15073.29 tokens per second)\n",
      "llama_perf_context_print:        load time =    2018.41 ms\n",
      "llama_perf_context_print: prompt eval time =    1131.30 ms /   141 tokens (    8.02 ms per token,   124.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24026.85 ms /   366 runs   (   65.65 ms per token,    15.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   25211.86 ms /   507 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --seed 42 \\\n",
    "    --color \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89e410-0143-4055-9639-4f3a2387930c",
   "metadata": {},
   "source": [
    "### Number of Tokens to Predict\n",
    "\n",
    "The `-n N, --predict N` (default: -1) controls the number of tokens the model generates in response to the input prompt. By adjusting this value, you can influence the length of the generated text. A higher value will result in longer text, while a lower value will produce shorter text.\n",
    "\n",
    "Even though all models have a finite context window, a value of -1 will enable *infinite* text generation. How? When the context window is full, some of the earlier tokens (half of the tokens after `--keep`) will be discarded. The context must then be re-evaluated before generation can resume. On large models and/or large context windows, this can result in a significant pause in output. If the output delay is undesirable, a value of -2 will stop generation immediately when the context is filled.\n",
    "\n",
    "It is important to note that the generated text may be shorter than the specified number of tokens if an End-of-Sequence (EOS) token or a reverse prompt is encountered. In interactive mode, text generation will pause and control will be returned to the user. In non-interactive mode, the program will end. In both cases, the text generation may stop before reaching the specified `--predict` value. If you want the model to keep going without ever producing End-of-Sequence on its own, you can use the `--ignore-eos` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d133e58-15ff-4119-aaef-4b59256e26fb",
   "metadata": {},
   "source": [
    "#### Basic `--predict` example\n",
    "\n",
    "```bash\n",
    "llama-cli --model \"$MODEL\" --predict 10 --prompt \"What is the meaning of life?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12486c3-cdb9-47bb-b9a7-c3f65df82a64",
   "metadata": {},
   "source": [
    "#### \"until context filled\" text generation example\n",
    "\n",
    "```bash\n",
    "llama-cli --model \"$MODEL\" --ctx-size 10 --predict -2 --prompt \"What is the meaning of life?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e90bd-deac-4bf7-a2c3-fa6a402a1592",
   "metadata": {},
   "source": [
    "#### \"Infinite\" text generation example\n",
    "\n",
    "```bash\n",
    "llama-cli --model \"$MODEL\" --ctx-size 10 --predict -1 --prompt \"What is the meaning of life?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569217e-1faa-4cd6-92dc-a2b9fdd77af5",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "-   `--temp N`: Adjust the randomness of the generated text (default: 0.8).\n",
    "\n",
    "Temperature is a hyperparameter that controls the randomness of the generated text. It affects the probability distribution of the model's output tokens. A higher temperature (e.g., 1.5) makes the output more random and creative, while a lower temperature (e.g., 0.5) makes the output more focused, deterministic, and conservative. The default value is 0.8, which provides a balance between randomness and determinism. At the extreme, a temperature of 0 will always pick the most likely next token, leading to identical outputs in each run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ec8d2-e59a-411b-9937-ea72d38cb8a0",
   "metadata": {},
   "source": [
    "#### Default `--temp` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4ffc1720-8fd5-4d4c-bd4d-46f0cced0279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 1858271751\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not\u001b[0m explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "**Tweet 1/5**\n",
      "\n",
      "Hey there, code enthusiasts! 💻✨ Ever wondered how apps remember your preferences or how websites suggest products you might love? That's the magic of Computer Science! 💫 It's the science behind the algorithms that power our digital world. #CSisEverywhere #TechChat\n",
      "\n",
      "**Tweet 2/5**\n",
      "\n",
      "Did you know? There are over 23 million computer science professionals worldwide! 🌎 With high demand for skilled coders, it's a field with incredible growth potential. 📈 If you're curious about shaping the future, CS is definitely worth exploring! #CareerGoals #TechLife\n",
      "\n",
      "**Tweet 3/5**\n",
      "\n",
      "Beyond just writing code, Computer Science is about problem-solving. 💪 From designing efficient algorithms to building complex software systems, it's about creating innovative solutions to real-world challenges. 🌍 Think of it as building the foundations for the future! #Innovation #CSProblemSolving\n",
      "\n",
      "**Tweet 4/5**\n",
      "\n",
      "Fun fact: The first computer program was written in 1842 by Ada Lovelace! 💻✨ Her work laid the groundwork for modern-day programming. 📚 #WomenInTech #AdaLovelace\n",
      "\n",
      "**Tweet 5/5**\n",
      "\n",
      "Computer science is more than just lines of code. 💻 It's about understanding the underlying concepts, solving complex problems, and creating impactful solutions. 🌍 If you're ready to embark on a journey of endless possibilities, explore the world of Computer Science! #CSJourney #TechEnthusiast [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      29.47 ms /   466 runs   (    0.06 ms per token, 15814.84 tokens per second)\n",
      "llama_perf_context_print:        load time =    1860.84 ms\n",
      "llama_perf_context_print: prompt eval time =    1135.05 ms /   141 tokens (    8.05 ms per token,   124.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21173.36 ms /   324 runs   (   65.35 ms per token,    15.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   22356.97 ms /   465 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli --model \"$1\" --temp 0.8 --color --file ../prompts/engaging-twitter-thread.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba68e0e-5a0b-4ce9-bbd2-5b3346498ecc",
   "metadata": {},
   "source": [
    "#### Low `--temp` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37aaa2b5-c8de-48d4-b52c-afa2d1f5493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 2033017665\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.400\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun\u001b[0m facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "**Tweet 1:**\n",
      "\n",
      "Hey there, code enthusiasts! 💻✨ Ever wondered how apps remember your preferences or how websites suggest products you might love? That's the magic of **Computer Science** in action! 💪 It's the science of creating intelligent systems that can learn and adapt to your needs. #CSIsEverywhere #LearningNeverStops\n",
      "\n",
      "**Tweet 2:**\n",
      "\n",
      "Did you know? 🤯 Over **2 million computer science jobs** were added in the US in 2023 alone! 💻 This field is booming with opportunities for creative minds and problem-solvers. 💡 If you're curious about shaping the future, CS is definitely worth exploring! #FutureOfWork #CSCareers #TechGrowth\n",
      "\n",
      "**Tweet 3:**\n",
      "\n",
      "Beyond just coding, Computer Science is about **building algorithms** that can tackle complex problems. 🧠 Think of it like creating step-by-step instructions for a computer to follow. These algorithms can analyze data, detect patterns, and make predictions. #AlgorithmicThinking #DataDrivenDecisions #CSApplications\n",
      "\n",
      "**Tweet 4:**\n",
      "\n",
      "I've personally witnessed how CS can transform industries. From healthcare to transportation, from entertainment to education, the applications are endless. 🌎 This field has the power to solve real-world problems and improve our daily lives. #CSImpact #Innovation #TechForGood [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      26.20 ms /   421 runs   (    0.06 ms per token, 16071.77 tokens per second)\n",
      "llama_perf_context_print:        load time =    2031.56 ms\n",
      "llama_perf_context_print: prompt eval time =    1131.14 ms /   141 tokens (    8.02 ms per token,   124.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17992.20 ms /   279 runs   (   64.49 ms per token,    15.51 tokens per second)\n",
      "llama_perf_context_print:       total time =   19165.60 ms /   420 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli --model \"$1\" --temp 0.4 --color --file ../prompts/engaging-twitter-thread.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b25125-44be-45d5-96c4-1816857fb59e",
   "metadata": {},
   "source": [
    "#### High `--temp` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f583ef51-1c95-4f29-b85b-aff440d87fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 2862417243\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.400\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone\u001b[0m of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation. \n",
      "\n",
      "**Tweet 1/5**\n",
      "\n",
      "Hey there, code lovers! 💻 Ever wondered what makes the apps you use tick? 🤔 That's the magic of Computer Science! 💪 It's the art of crafting algorithms and building software that shapes our digital world 🌎 From Instagram filters to online banking, CS is everywhere! #CSFun #TechUnbound\n",
      "\n",
      "**Tweet 2/5**\n",
      "\n",
      "Did you know? 🤯 The global IT industry contributes over $3.5 trillion to the US economy annually 💻 That's more than the entire manufacturing sector combined! 🤯 So, if you're looking for a career that makes a real difference, CS is definitely worth exploring 😉 #CSCareer #FutureOfWork\n",
      "\n",
      "**Tweet 3/5**\n",
      "\n",
      "There's a myth that you need to be a math whiz to be a good computer scientist 📚 But that's just not true! 💡 Creativity, problem-solving skills, and a knack for turning ideas into code are just as important. 💡 And the best part? Anyone can learn these skills with the right resources! #CSForEveryone #LearningIsFun\n",
      "\n",
      "**Tweet 4/5**\n",
      "\n",
      "One of the coolest things about CS is the endless possibilities 💫 You can build anything from mobile games to space exploration software 🤯 And the demand for skilled CS professionals is only going to grow in the coming years. 🚀 #CSImpact #TheFutureIsNow\n",
      "\n",
      "**Tweet 5/5**\n",
      "\n",
      "So, if you're curious about the power of technology and want to shape the future, consider exploring the world of Computer Science! 💻 There are plenty of resources available online and in your community to get you started. Let's build something amazing together! #CSCommunity #TechIsEverywhere [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      32.85 ms /   507 runs   (    0.06 ms per token, 15434.26 tokens per second)\n",
      "llama_perf_context_print:        load time =    1947.13 ms\n",
      "llama_perf_context_print: prompt eval time =    1137.36 ms /   141 tokens (    8.07 ms per token,   123.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23653.09 ms /   365 runs   (   64.80 ms per token,    15.43 tokens per second)\n",
      "llama_perf_context_print:       total time =   24841.23 ms /   506 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli --model \"$1\" --temp 1.4 --color --file ../prompts/engaging-twitter-thread.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fca20a-29da-4883-847d-943cd29a4c78",
   "metadata": {},
   "source": [
    "### Repeat Penalty\n",
    "\n",
    "The `--repeat-penalty` option helps prevent the model from generating repetitive or monotonous text. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. The default value is 1 (which means no penalty).\n",
    "\n",
    "The `--repeat-last-n` option controls the number of tokens in the history to consider for penalizing repetition. A larger value will look further back in the generated text to prevent repetitions, while a smaller value will only consider recent tokens. A value of 0 disables the penalty, and a value of -1 sets the number of tokens considered equal to the context size, `--ctx-size`. The default value is 64. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68fc3167-4d73-4136-94d6-685d0f753907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 760026873\n",
      "sampler params: \n",
      "\trepeat_last_n = 128, repeat_penalty = 1.500, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language.\u001b[0m You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "**Tweet #1:**\n",
      "Hey there! 👋 Ever wondered how your phone can translate languages or suggest songs based on mood? That's magic powered by **Computer science (CS)** 💪 It deals with building & designing digital systems that shape our daily lives ✨ From apps you love  to complex algorithms solving real-world problems - CS is everywhere 😉 Join us for a journey into this fascinating field of tech 💻✨#CompSciLove\n",
      "\n",
      "\n",
      "<strong>Note:</strong> This prompt asks me not self reference so I am unable provide any personal experiences, opinions ,or insights in my response as requested [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      69.41 ms /   261 runs   (    0.27 ms per token,  3760.05 tokens per second)\n",
      "llama_perf_context_print:        load time =    1765.60 ms\n",
      "llama_perf_context_print: prompt eval time =    1132.28 ms /   141 tokens (    8.03 ms per token,   124.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7643.48 ms /   119 runs   (   64.23 ms per token,    15.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    8852.79 ms /   260 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --repeat-penalty 1.5 \\\n",
    "    --repeat-last-n 128 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd40af-3201-4d6b-96b2-a65e9a431689",
   "metadata": {},
   "source": [
    "### Top-K Sampling\n",
    "\n",
    "Top-k sampling is a text generation method that selects the next token only from the `--top-k` most likely tokens predicted by the model. It helps reduce the risk of generating low-probability or nonsensical tokens, but it may also limit the diversity of the output. A higher value for top-k (e.g., 100) will consider more tokens and lead to more diverse text, while a lower value (e.g., 10) will focus on the most probable tokens and generate more conservative text. The default value is 40.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "652ee26a-2fe2-42bb-9bcc-bfe143e6e84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 1249473218\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 32, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a\u001b[0m large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation. \n",
      "\n",
      "**Tweet 1/5**\n",
      "\n",
      "Hey there, code enthusiasts! 💻 Ever wondered what makes those apps you love tick? That's the magic of **Computer Science** 🧙‍♀️! It's the science of designing, building, and maintaining the software and hardware that shapes our digital world 🌍. #CSisFun #TechLife\n",
      "\n",
      "**Tweet 2/5**\n",
      "\n",
      "Did you know? The global tech industry is projected to reach **$7.3 trillion in 2030** 🚀! That's a massive opportunity for those with the right skills 💪. And guess what? Computer Science opens doors to a wide range of exciting careers. #CareerGoals #TechCareers\n",
      "\n",
      "**Tweet 3/5**\n",
      "\n",
      "Beyond just writing code 💻, Computer Science is about problem-solving 💪. It's about creating innovative solutions to complex challenges 💡. From developing AI-powered robots 🤖 to designing secure online systems 🔒, the possibilities are endless! #Innovation #AI #Cybersecurity\n",
      "\n",
      "**Tweet 4/5**\n",
      "\n",
      "I had the privilege of working on a project that used **virtual reality to help children learn math** 📚! The power of technology to transform learning is truly incredible 🤯. #EdTech #FutureOfWork #VRforLearning\n",
      "\n",
      "**Tweet 5/5**\n",
      "\n",
      "So, are you curious to explore the world of Computer Science? 🤔 There are plenty of online resources, courses, and communities to get started 💻. Join the movement and let's build the future together! #CSCommunity #TechEducation #JoinTheConversation [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      33.68 ms /   470 runs   (    0.07 ms per token, 13953.63 tokens per second)\n",
      "llama_perf_context_print:        load time =    1751.28 ms\n",
      "llama_perf_context_print: prompt eval time =    1126.72 ms /   141 tokens (    7.99 ms per token,   125.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21196.33 ms /   328 runs   (   64.62 ms per token,    15.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   22375.40 ms /   469 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-k 32 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bec410a-818d-4a2e-aea2-e84231d744b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 1551997244\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 10, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only\u001b[0m in the English language. You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "## Thread: The Fascinating World of Computer Science 💻📚\n",
      "\n",
      "Ever wondered how apps remember your preferences? Or how websites load in seconds? That's the magic of Computer Science! 💪\n",
      "\n",
      "It's the building block of everything digital, from the apps we use daily to the robots of the future. 🤖\n",
      "\n",
      "Did you know? ➡️ Over 23 million people worldwide are employed in Computer Science! 🤯 That's a huge industry with endless opportunities. 💻\n",
      "\n",
      "I fell in love with this field because it allows me to solve problems, create innovative solutions, and make a real impact on the world. 🌎\n",
      "\n",
      "Computer Science is all about problem-solving 💡. It's about using logic, creativity, and technology to design and implement solutions to complex challenges. 🧠\n",
      "\n",
      "There are many branches of Computer Science, each with its own focus and applications. 📚 Cybersecurity, Artificial Intelligence, Data Science... the list goes on! 🤯\n",
      "\n",
      "So, if you're curious about the digital world and want to make a real difference, consider exploring Computer Science. It's a journey filled with endless possibilities and the potential to shape the future! 🚀 #ComputerScience #TechLife #FutureIsHere #JoinTheConversation [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      20.75 ms /   398 runs   (    0.05 ms per token, 19177.95 tokens per second)\n",
      "llama_perf_context_print:        load time =    1729.98 ms\n",
      "llama_perf_context_print: prompt eval time =    1145.92 ms /   141 tokens (    8.13 ms per token,   123.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16575.67 ms /   256 runs   (   64.75 ms per token,    15.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   17756.02 ms /   397 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-k 10 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97acf190-4394-4582-aae8-669bdb361bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 1102286239\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 100, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter\u001b[0m influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "**Tweet 1/5**\n",
      "\n",
      "Hey there, code enthusiasts! 💻✨ Did you know there are over 23 million computer scientists globally? 🤯 That's a whole lotta minds solving problems and shaping the future! #CSCommunity #TechLife\n",
      "\n",
      "**Tweet 2/5**\n",
      "\n",
      "The beauty of computer science is its versatility. 💻 From building mind-blowing apps to analyzing complex data, there's a niche for everyone. 🧠 From web development to cybersecurity, the possibilities are endless! #CSCareers #TechTrends\n",
      "\n",
      "**Tweet 3/5**\n",
      "\n",
      "Did you know the first computer scientist was Ada Lovelace? 📚 Women have always been trailblazers in the field. Today, over 40% of computer scientists are women! 💪 #WomenInSTEM #CSHistory\n",
      "\n",
      "**Tweet 4/5**\n",
      "\n",
      "Computer science isn't just about coding. 💻 It's about problem-solving, creativity, and teamwork. 🤝 You'll learn to think critically, build innovative solutions, and collaborate with others to bring your ideas to life. #CSSkills #Innovation\n",
      "\n",
      "**Tweet 5/5**\n",
      "\n",
      "So if you're curious about shaping the future and making a real impact, consider exploring computer science! 💻 There are countless opportunities waiting for you. #CSLife #TechFuture #JoinTheConversation [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      30.54 ms /   422 runs   (    0.07 ms per token, 13818.40 tokens per second)\n",
      "llama_perf_context_print:        load time =    1575.63 ms\n",
      "llama_perf_context_print: prompt eval time =    1130.34 ms /   141 tokens (    8.02 ms per token,   124.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18060.41 ms /   280 runs   (   64.50 ms per token,    15.50 tokens per second)\n",
      "llama_perf_context_print:       total time =   19236.56 ms /   421 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-k 100 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592f98d-9a51-458b-b8c8-5171a85cddf6",
   "metadata": {},
   "source": [
    "### Top-P Sampling\n",
    "\n",
    "-   `--top-p N`: Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P (default: 0.9).\n",
    "\n",
    "Top-p sampling, also known as nucleus sampling, is another text generation method that selects the next token from a subset of tokens that together have a cumulative probability of at least p. This method provides a balance between diversity and quality by considering both the probabilities of tokens and the number of tokens to sample from. A higher value for top-p (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. The default value is 0.9.\n",
    "\n",
    "Example usage: `--top-p 0.95`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4215bdb-cdef-4d9e-a875-f41c07c7131a",
   "metadata": {},
   "source": [
    "#### Default `--top-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3dd5ac7-fef8-4449-a75c-dd6918826936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 904173040\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.900, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain\u001b[0m what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "## Thread: The Magical World of Computer Science 💻✨\n",
      "\n",
      "Ever wondered how apps respond instantly to your touch? Or how websites load in a flash? That's the magic of **Computer Science** 💻!\n",
      "\n",
      "It's the science of designing, building, and maintaining the software and systems that power our digital world 💻📚. \n",
      "\n",
      "Did you know? 🤯\n",
      "\n",
      "- 82% of US jobs involving STEM fields require computer science skills 💻\n",
      "- The global software market is expected to reach $1.3 trillion by 2030 🚀\n",
      "\n",
      "### It's more than just coding... 💻\n",
      "\n",
      "Computer science involves:\n",
      "\n",
      "- **Problem-solving** 💡\n",
      "- **Logical thinking** 🧠\n",
      "- **Creativity** 🎨\n",
      "- **Collaboration** 🤝\n",
      "\n",
      "These are skills that will benefit you in any field, not just tech! 💪\n",
      "\n",
      "**What's your dream in computer science? Share your thoughts below! 👇 #ComputerScience #TechLife #DigitalFuture** [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      19.25 ms /   353 runs   (    0.05 ms per token, 18332.90 tokens per second)\n",
      "llama_perf_context_print:        load time =    1716.48 ms\n",
      "llama_perf_context_print: prompt eval time =    1131.20 ms /   141 tokens (    8.02 ms per token,   124.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13688.10 ms /   211 runs   (   64.87 ms per token,    15.41 tokens per second)\n",
      "llama_perf_context_print:       total time =   14851.37 ms /   352 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-p 0.9 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b65992-578b-4b0e-a7c9-a023257ffca8",
   "metadata": {},
   "source": [
    "#### Low `--top-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c62c1bfc-ac22-4fa8-8c57-6945016dafc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 1014519072\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.500, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference.\u001b[0m Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "**Tweet 1/5**\n",
      "\n",
      "Hey there, code enthusiasts! 💻✨ Ever wondered how apps remember your preferences or how websites suggest products you might love? That's the magic of Computer Science! 📚 It's the foundation of everything that makes technology personalized and interactive. #CSIsEverywhere #TechMagic\n",
      "\n",
      "**Tweet 2/5**\n",
      "\n",
      "Did you know? 🤯 Over 23 million people worldwide are employed in Computer Science-related fields! 💻 That's a massive industry with endless opportunities for creative minds. 💡 If you're curious about shaping the future, CS is definitely worth exploring. #CSCareers #FutureTech\n",
      "\n",
      "**Tweet 3/5**\n",
      "\n",
      "Beyond just coding, Computer Science is about problem-solving. 💪 It's about designing algorithms that can tackle complex challenges, building systems that can handle vast amounts of data, and creating innovative solutions to real-world problems. #CSProblemSolving #Innovation\n",
      "\n",
      "**Tweet 4/5**\n",
      "\n",
      "Fun fact: 💡 The first computer program was written in 1842 by Ada Lovelace, a trailblazer in computer science! 👩‍💻 Her work laid the groundwork for modern-day programming languages. #AdaLovelace #ComputerScienceHistory\n",
      "\n",
      "**Tweet 5/5**\n",
      "\n",
      "So, are you curious about the world of Computer Science? 💻 It's a journey filled with challenges, rewards, and endless possibilities. Join the conversation and share your thoughts on how CS impacts your life! #CSCommunity #TechTalk [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      28.80 ms /   459 runs   (    0.06 ms per token, 15939.71 tokens per second)\n",
      "llama_perf_context_print:        load time =    1919.87 ms\n",
      "llama_perf_context_print: prompt eval time =    1131.24 ms /   141 tokens (    8.02 ms per token,   124.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20418.50 ms /   317 runs   (   64.41 ms per token,    15.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   21595.23 ms /   458 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-p 0.5 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88066c66-c556-4c51-b016-431fb4532fd8",
   "metadata": {},
   "source": [
    "#### High `--top-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e95ba3c0-8646-4597-8d92-ea3dc7c7f621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 2007731864\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual\u001b[0m, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "**Tweet 1:**\n",
      "\n",
      "Hey there, code enthusiasts! 💻 Ever wondered what makes apps run so smoothly or how websites load in a flash? Enter: Computer Science! 💪 It's the magic behind the tech we take for granted. 🧙‍♀️ #CSUnbound #TechMagic\n",
      "\n",
      "**Tweet 2:**\n",
      "\n",
      "Did you know? 🤯 Over 23 million computer science graduates worldwide contribute to shaping the future 🚀. That's a lot of innovative minds working on everything from developing AI algorithms to designing the next generation of gaming consoles! #CSIsEverywhere #FutureOfWork\n",
      "\n",
      "**Tweet 3:**\n",
      "\n",
      "There's more to CS than just coding! 💻 It's about problem-solving, creativity, and building logical structures. 💪 It's about understanding how data works and making sense of the world through technology. 🌎 #CSIsMoreThanCode #TheLogicalMind\n",
      "\n",
      "**Tweet 4:**\n",
      "\n",
      "Personally, I've found CS to be an incredibly rewarding journey. 🤩 The feeling of building something from scratch and seeing it come to life is unmatched. 💫 If you're curious about technology and want to make a real impact, CS might be your calling! #JoinTheJourney #CSCareers\n",
      "\n",
      "**Tweet 5:**\n",
      "\n",
      "Did you know? 💡 The first computer science degree was awarded in 1949! 💻 It's been an ever-evolving field, constantly pushing boundaries and shaping our world. #CSHistory #EverEvolvingField\n",
      "\n",
      "**Tweet 6:**\n",
      "\n",
      "So, what are your thoughts on Computer Science? 💻 Share your experiences, questions, or dreams in the comments! 👇 #CSCommunity #TheFutureIsTech #LetsTalkCS [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      37.79 ms /   494 runs   (    0.08 ms per token, 13072.59 tokens per second)\n",
      "llama_perf_context_print:        load time =    1665.69 ms\n",
      "llama_perf_context_print: prompt eval time =    1130.17 ms /   141 tokens (    8.02 ms per token,   124.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23281.70 ms /   352 runs   (   66.14 ms per token,    15.12 tokens per second)\n",
      "llama_perf_context_print:       total time =   24472.78 ms /   493 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-p 0.95 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf85b2e-c110-4dd7-84c6-952048fb55c4",
   "metadata": {},
   "source": [
    "### Min-P Sampling\n",
    "\n",
    "The `--min-p` sampling method sets a minimum base probability threshold for token selection and aims to ensure a balance of quality and variety in the generated text. The `--min-p` method was designed as an alternative to `--top-p`. The parameter *p* represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with *p*=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. The default value is 0.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193778cd-271f-47be-9996-1b0d3467e38e",
   "metadata": {},
   "source": [
    "#### Default `--min-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04d5e26b-12c9-44d4-82c4-153d832265a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 73609509\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.100, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large\u001b[0m following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "## Thread: The Magic of Computer Science 💻✨\n",
      "\n",
      "Imagine a world where problems solve themselves, machines learn like humans, and creativity meets technology. That's the power of **Computer Science** 💪! \n",
      "\n",
      "Did you know? ➡️ 23% of the fastest-growing jobs in the US require Computer Science skills 💻. \n",
      "\n",
      "It's about building the future, shaping how we interact with technology, and making sense of data with algorithms ✨. \n",
      "\n",
      "## The Creative Side 🎨\n",
      "\n",
      "Computer science isn't just about code and algorithms. It's about **solving problems creatively**, building innovative products, and finding elegant solutions to complex situations. \n",
      "\n",
      "For example: ➡️ AI can translate languages in real-time 🤯, AR can bring historical artifacts to life 🌌, and blockchain technology can revolutionize finances 💰.\n",
      "\n",
      "## It's Accessible! 📚\n",
      "\n",
      "The best part? Computer science is open to everyone! ➡️ 40% of computer science degrees go to women 👩‍💻, and 20% are from underrepresented racial and ethnic groups 👨‍👩‍👧‍👦. \n",
      "\n",
      "There are plenty of resources and online courses available to get started 💻📚. \n",
      "\n",
      "**What's your take on the future of Computer Science? Share your thoughts below! 👇 #CSRevolution #TechForGood #TheFutureIsHere #JoinTheConversation** [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      28.37 ms /   435 runs   (    0.07 ms per token, 15330.94 tokens per second)\n",
      "llama_perf_context_print:        load time =    2531.79 ms\n",
      "llama_perf_context_print: prompt eval time =    1126.58 ms /   141 tokens (    7.99 ms per token,   125.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19150.07 ms /   293 runs   (   65.36 ms per token,    15.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   20322.09 ms /   434 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-p 0.9 \\\n",
    "    --min-p 0.1 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40dbc8-0e28-4b8f-a45d-6bb77bd41e53",
   "metadata": {},
   "source": [
    "#### Low `--min-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a1b26be-5553-4857-a023-cc565c7277c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 1466269480\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\u001b[0m\n",
      "\n",
      "## Thread: The Magic of Computer Science 💻📚✨\n",
      "\n",
      "Ever wondered how apps remember your preferences or how websites know exactly what you're searching for? That's the magic of **Computer Science (CS)**! 🤯\n",
      "\n",
      "CS is about building the technology that shapes our world. It's about writing code, designing algorithms, and tackling complex problems using logic and creativity 💪.\n",
      "\n",
      "Did you know? ➡️ \n",
      "- 62% of US jobs requiring STEM skills are in computer and mathematical fields 💻\n",
      "- The average salary for a CS graduate in the US is $107,730 💰\n",
      "\n",
      "CS impacts everything from healthcare to transportation to entertainment 👨‍💻👩‍💻. It's the driving force behind the innovations we take for granted. 🤯\n",
      "\n",
      "Join the conversation and share your thoughts about the future of CS! What are your hopes and dreams? #ComputerScience #STEM #TechInnovation [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      18.46 ms /   336 runs   (    0.05 ms per token, 18201.52 tokens per second)\n",
      "llama_perf_context_print:        load time =    1797.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1132.28 ms /   141 tokens (    8.03 ms per token,   124.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12484.22 ms /   194 runs   (   64.35 ms per token,    15.54 tokens per second)\n",
      "llama_perf_context_print:       total time =   13646.65 ms /   335 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-p 0.9 \\\n",
    "    --min-p 0.05 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b3064-8816-4f96-8a76-cb4fe9c0b3d2",
   "metadata": {},
   "source": [
    "#### High `--min-p` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "723a03d3-19cb-48df-968e-b6654ba050af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 3767110855\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.900, min_p = 0.200, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone\u001b[0m of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "**Tweet 1/5**\n",
      "\n",
      "Hey there, code enthusiasts! 💻 Ever wondered how apps seamlessly translate languages or suggest music you'll love? That's the magic of Computer Science! 💪 It's about building intelligent systems that understand and interact with humans like magic 💫 #CSIsEverywhere #TechMagic\n",
      "\n",
      "**Tweet 2/5**\n",
      "\n",
      "Did you know? 🤯 There are over 1 billion active mobile devices globally, and each one runs on computer science! 🤯 From the apps you use to the operating systems, CS is the backbone of our digital world 📱 #TechRevolution #CSInTheRealWorld\n",
      "\n",
      "**Tweet 3/5**\n",
      "\n",
      "One of the coolest things about CS is its vast potential. 🤯 From creating self-driving cars to exploring space, the possibilities are endless! 🚀 #FutureTech #CSForTheWin\n",
      "\n",
      "**Tweet 4/5**\n",
      "\n",
      "Personally, I love how CS empowers people to solve real-world problems. 🌎 From developing healthcare apps to tackling climate change, the power of code is boundless 🌎 #CSForGood #MakingADifference\n",
      "\n",
      "**Tweet 5/5**\n",
      "\n",
      "So, are you curious about the world of computer science? 💻 It's a journey filled with challenges, creativity, and endless possibilities. 💫 Join the conversation and let's explore the wonders of tech together! #CSCommunity #TechJourney #LetsTalkCS [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      27.15 ms /   435 runs   (    0.06 ms per token, 16021.51 tokens per second)\n",
      "llama_perf_context_print:        load time =    1831.47 ms\n",
      "llama_perf_context_print: prompt eval time =    1136.34 ms /   141 tokens (    8.06 ms per token,   124.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18995.89 ms /   293 runs   (   64.83 ms per token,    15.42 tokens per second)\n",
      "llama_perf_context_print:       total time =   20174.87 ms /   434 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --top-p 0.9 \\\n",
    "    --min-p 0.2 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f5421f5-c7b0-4a86-b83d-03ef3af29246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 2745326677\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 2\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\u001b[0m\n",
      "\n",
      "**Tweet 1/5**\n",
      "\n",
      "Hey there, code enthusiasts! 💻 Ever wondered how apps seamlessly process data or how websites load in an instant? That's the magic of Computer Science! 💪 It's the building block of our digital world, shaping everything from entertainment to healthcare. 🌍 #CSIsEverywhere #DigitalEra\n",
      "\n",
      "**Tweet 2/5**\n",
      "\n",
      "Did you know? 🤯 Over 75 million people globally are employed in Computer Science-related fields! 💻 This growing industry offers diverse opportunities for creative minds and problem-solvers. 💡 If you're curious about shaping the future, CS is definitely worth exploring! #FutureOfWork #TechCareers\n",
      "\n",
      "**Tweet 3/5**\n",
      "\n",
      "Beyond just coding, Computer Science is about understanding algorithms, data structures, and operating systems. 💻 These concepts are like building blocks for creating efficient and innovative solutions. 💪 It's a field where theory meets practice, and every new discovery leads to groundbreaking advancements. #ProblemSolving #Innovation\n",
      "\n",
      "**Tweet 4/5**\n",
      "\n",
      "Fun fact: 💡 The first computer scientist was Ada Lovelace, a trailblazer who designed algorithms for a mechanical computing machine way back in the 19th century! 💻 Her contributions paved the way for modern-day computing. #WomenInSTEM #HistoricalCS\n",
      "\n",
      "**Tweet 5/5**\n",
      "\n",
      "Computer Science opens doors to a world of possibilities. 🌎 From building autonomous robots to exploring the depths of space, the applications are limitless. 🌌 If you're looking for a fulfilling career that makes a real difference, CS is definitely worth considering! #CSLife #JoinTheConversation [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      31.76 ms /   481 runs   (    0.07 ms per token, 15143.88 tokens per second)\n",
      "llama_perf_context_print:        load time =    3547.38 ms\n",
      "llama_perf_context_print: prompt eval time =    1142.52 ms /   141 tokens (    8.10 ms per token,   123.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21981.09 ms /   339 runs   (   64.84 ms per token,    15.42 tokens per second)\n",
      "llama_perf_context_print:       total time =   23174.22 ms /   480 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --keep 1.0 \\\n",
    "    --tfs 1.0 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a589750-bcec-4af3-b27c-4be372668e50",
   "metadata": {},
   "source": [
    "### Locally Typical Sampling\n",
    "\n",
    "Locally typical sampling, `--typical` promotes the generation of contextually coherent and diverse text by sampling tokens that are typical or expected based on the surrounding context. By setting the parameter $p$ between 0 and 1, you can control the balance between producing text that is locally coherent and diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac0446-5e83-403c-8b66-5533ce721567",
   "metadata": {},
   "source": [
    "#### Default `--typical` example\n",
    "\n",
    "The default value of 1 disables locally typical sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1aabdf-5654-4a80-9d27-e3acd88d31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --typical 1.0 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f656c-8fe2-424e-9c95-2afc455ebf57",
   "metadata": {},
   "source": [
    "#### Typical `--typical` example\n",
    "\n",
    "A value closer to 1 will promote more contextually coherent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47d20679-77f5-4033-b9fd-c86d11687f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 2951066957\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 0.900, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English\u001b[0m language. You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "## Thread: The Superpowers of Computer Science 💪💻\n",
      "\n",
      "Ever wondered how apps remember your preferences, websites load in a flash, or robots can perform complex tasks? That's the magic of **Computer Science**! 💻✨\n",
      "\n",
      "It's the science of designing, building, and operating digital systems. Think of it as the superpower that brings technology to life! 💫\n",
      "\n",
      "Did you know? 🤯\n",
      "\n",
      "- The global tech industry employs over **23 million people** worldwide! 🌎\n",
      "- By 2030, the field is expected to grow by **22%**, creating millions of new opportunities. 📈\n",
      "\n",
      "Computer science is more than just coding. It's about solving problems, creating innovative solutions, and shaping the future of technology. 🌍\n",
      "\n",
      "There's a place for everyone in this incredible field, regardless of background or gender. So if you're curious about technology, logic, and making a real impact, consider exploring Computer Science! 💻📚\n",
      "\n",
      "#CSisAwesome #TechLife #FutureIsNow #JoinTheConversation 💬 [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      20.21 ms /   365 runs   (    0.06 ms per token, 18055.90 tokens per second)\n",
      "llama_perf_context_print:        load time =    3451.53 ms\n",
      "llama_perf_context_print: prompt eval time =    1128.57 ms /   141 tokens (    8.00 ms per token,   124.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14321.78 ms /   223 runs   (   64.22 ms per token,    15.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   15483.71 ms /   364 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --typical 0.9 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb2804-e83c-492e-86cb-62b6836bc12f",
   "metadata": {},
   "source": [
    "#### Low `--typical` example\n",
    "\n",
    "A `--typical` value closer to 0 will promote more diverse tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7d35993f-0302-49a0-ac2a-0caa4296deb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 4098684404\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 0.250, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are\u001b[0m a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "## Thread: 1/5\n",
      "\n",
      "Hey there, code enthusiasts! 💻✨ Ever wondered how apps remember your preferences or how websites suggest products you might love? That's the magic of computer science! 💪 It's the science of building intelligent machines that can learn, adapt, and solve problems. #CSIsEverywhere #LearningMachine\n",
      "\n",
      "## Thread: 2/5\n",
      "\n",
      "Did you know there are over 23 million computer science students worldwide? 🤯 That's a huge community of creative minds shaping the future. 🌍 And the best part? Anyone can learn the basics of CS! With online courses, free tutorials, and interactive games, there's something for everyone. #CSForEveryone #EmpoweringYouth\n",
      "\n",
      "## Thread: 3/5\n",
      "\n",
      "Computer science isn't just about coding. 💻 It's about problem-solving, creativity, and collaboration. 🧠 Teams of CS experts tackle challenges across industries, from designing autonomous vehicles to protecting our privacy online. 🚗🔒 #CSApplications #TheFutureIsHere\n",
      "\n",
      "## Thread: 4/5\n",
      "\n",
      "Fun fact: the first computer science degree was awarded in 1963! 🤯 That's just over 60 years ago! 🤯 And since then, the field has grown exponentially, with new innovations emerging every day. #CSEvolution #EverEvolving\n",
      "\n",
      "## Thread: 5/5\n",
      "\n",
      "So, are you curious about the world of computer science? 💻 It's an exciting journey filled with endless possibilities. Join the conversation and share your thoughts on the future of this innovative field! #CSCommunity #JoinTheDiscussion [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =      31.49 ms /   478 runs   (    0.07 ms per token, 15179.42 tokens per second)\n",
      "llama_perf_context_print:        load time =    2354.31 ms\n",
      "llama_perf_context_print: prompt eval time =    1129.27 ms /   141 tokens (    8.01 ms per token,   124.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21873.39 ms /   336 runs   (   65.10 ms per token,    15.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   23053.39 ms /   477 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --typical 0.25 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed2635-b652-4e9a-89eb-ed87f7b2bdca",
   "metadata": {},
   "source": [
    "### Mirostat Sampling\n",
    "\n",
    "Mirostat is an algorithm that actively maintains the quality of generated text within a desired range during text generation. It aims to strike a balance between coherence and diversity, avoiding low-quality output caused by excessive repetition (boredom traps) or incoherence (confusion traps). To enable Mirostat sampling set `--mirostat` to 1 = Mirostat 1.0 or 2 = Mirostat 2.0. By default Mirostat sampling is disabled, `--mirostat 0`.\n",
    "\n",
    "The `--mirostat-lr` option sets the Mirostat learning rate (eta). The learning rate influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. The default value is `0.1`.\n",
    "\n",
    "The `--mirostat-ent` option sets the Mirostat target entropy (tau), which represents the desired perplexity value for the generated text. Adjusting the target entropy allows you to control the balance between coherence and diversity in the generated text. A lower value will result in more focused and coherent text, while a higher value will lead to more diverse and potentially less coherent text. The default value is `5.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433abca4-a249-463d-ba6b-43e916e56da1",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "811c2ca3-5401-4005-aa6f-bb27153312b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build: 3865 (00b7317e) with Apple clang version 16.0.0 (clang-1600.0.26.3) for arm64-apple-darwin24.0.0\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 254 tensors from ../models/gemma-1.1-7b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-7b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control-looking token: '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 24576\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.54 B\n",
      "llm_load_print_meta: model size       = 4.96 GiB (4.99 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-1.1-7b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   615.23 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5077.10 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  3584.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 3584.00 MiB, K (f16): 1792.00 MiB, V (f16): 1792.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   506.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    22.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 931\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 4\n",
      "\n",
      "system_info: n_threads = 4 (n_threads_batch = 4) / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 0 | \n",
      "\n",
      "sampler seed: 2840937485\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 2, mirostat_lr = 0.050, mirostat_ent = 3.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> temp -> mirostat-v2 \n",
      "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone of voice.\u001b[0m You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n",
      "**Tweet 1/5:**\n",
      "\n",
      "Hey there, code enthusiasts! 💻✨ Ever wondered how apps seamlessly translate your language into different languages? That's the magic of Natural Language Processing (NLP)! 💪 NLP algorithms analyze language patterns to understand meaning & translate seamlessly. #ComputerScience #NLPPower #LanguageTranslation 🌎\n",
      "\n",
      "**Tweet 2/5:**\n",
      "\n",
      "Did you know? 🤯 NLP has evolved significantly in the past decade! 🚀 With Machine Learning & Deep Learning, NLP models can now understand context & sentiment, leading to improved translation & understanding. #MLforNLP #DeepLearning #LanguageUnderstanding 💪\n",
      "\n",
      "**Tweet 3/5:**\n",
      "\n",
      "Beyond translation, NLP has numerous applications across industries. 💻 From sentiment analysis to text summarization, NLP helps businesses understand customer feedback & optimize content. 🤩 For example, chatbots powered by NLP provide personalized customer support. #NLPApplications #CustomerFeedback #ContentOptimization\n",
      "\n",
      "**Tweet 4/5:**\n",
      "\n",
      "Fun fact: NLP algorithms can even generate human-quality text! 🤯 This technology is known as Text-to-Text (T5) generation. 🤩 With T5, you can create unique stories, poems, or even code! #TextGeneration #NLPInnovation #CreativeAI 🎨\n",
      "\n",
      "**Tweet 5/5:**\n",
      "\n",
      "The future of NLP is bright! 💫 As the field continues to grow, we can expect even more groundbreaking applications. Stay tuned for more exciting developments in NLP! #NLPExploration #FutureTech #AIforGood ✨ [end of text]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_sampler_print:    sampling time =    4944.35 ms /   450 runs   (   10.99 ms per token,    91.01 tokens per second)\n",
      "llama_perf_context_print:        load time =    2032.12 ms\n",
      "llama_perf_context_print: prompt eval time =    1138.22 ms /   141 tokens (    8.07 ms per token,   123.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19937.24 ms /   308 runs   (   64.73 ms per token,    15.45 tokens per second)\n",
      "llama_perf_context_print:       total time =   26037.06 ms /   449 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$MODEL\"\n",
    "\n",
    "llama-cli \\\n",
    "    --model \"$1\" \\\n",
    "    --color \\\n",
    "    --mirostat 2 \\\n",
    "    --mirostat-lr 0.05 \\\n",
    "    --mirostat-ent 3.0 \\\n",
    "    --file ../prompts/engaging-twitter-thread.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da475842-8e5d-4eb7-bc63-cf96ac876357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6256957-6043-4bba-8f0c-92b199790d47",
   "metadata": {},
   "source": [
    "## 6. Performance Tuning and Memory Options\n",
    "\n",
    "These options help improve the performance and memory usage of the LLaMA models. By adjusting these settings, you can fine-tune the model's behavior to better suit your system's capabilities and achieve optimal performance for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a048cd5-2e6c-4c57-a667-ad78942fd660",
   "metadata": {},
   "source": [
    "### Number of Threads\n",
    "\n",
    "-   `-t N, --threads N`: Set the number of threads to use during generation. For optimal performance, it is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Using the correct number of threads can greatly improve performance.\n",
    "-   `-tb N, --threads-batch N`: Set the number of threads to use during batch and prompt processing. In some systems, it is beneficial to use a higher number of threads during batch processing than during generation. If not specified, the number of threads used for batch processing will be the same as the number of threads used for generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350ba86-1aa3-40da-a7de-2902a29c4851",
   "metadata": {},
   "source": [
    "### Mlock\n",
    "\n",
    "-   `--mlock`: Lock the model in memory, preventing it from being swapped out when memory-mapped. This can improve performance but trades away some of the advantages of memory-mapping by requiring more RAM to run and potentially slowing down load times as the model loads into RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c67d2-1bdf-4a0f-afe4-6ec6b4ad966e",
   "metadata": {},
   "source": [
    "### No Memory Mapping\n",
    "\n",
    "-   `--no-mmap`: Do not memory-map the model. By default, models are mapped into memory, which allows the system to load only the necessary parts of the model as needed. However, if the model is larger than your total amount of RAM or if your system is low on available memory, using mmap might increase the risk of pageouts, negatively impacting performance. Disabling mmap results in slower load times but may reduce pageouts if you're not using `--mlock`. Note that if the model is larger than the total amount of RAM, turning off mmap would prevent the model from loading at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afdf19c-7ab4-401c-aa0c-30390ca7d80b",
   "metadata": {},
   "source": [
    "### NUMA support\n",
    "\n",
    "-   `--numa distribute`: Pin an equal proportion of the threads to the cores on each NUMA node. This will spread the load amongst all cores on the system, utilitizing all memory channels at the expense of potentially requiring memory to travel over the slow links between nodes.\n",
    "-   `--numa isolate`: Pin all threads to the NUMA node that the program starts on. This limits the number of cores and amount of memory that can be used, but guarantees all memory access remains local to the NUMA node.\n",
    "-   `--numa numactl`: Pin threads to the CPUMAP that is passed to the program by starting it with the numactl utility. This is the most flexible mode, and allow arbitrary core usage patterns, for example a map that uses all the cores on one NUMA nodes, and just enough cores on a second node to saturate the inter-node memory bus.\n",
    "\n",
    " These flags attempt optimizations that help on some systems with non-uniform memory access. This currently consists of one of the above strategies, and disabling prefetch and readahead for mmap. The latter causes mapped pages to be faulted in on first access instead of all at once, and in combination with pinning threads to NUMA nodes, more of the pages end up on the NUMA node where they are used. Note that if the model is already in the system page cache, for example because of a previous run without this option, this will have little effect unless you drop the page cache first. This can be done by rebooting the system or on Linux by writing '3' to '/proc/sys/vm/drop_caches' as root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e29fe2-d43b-4289-b1e2-a10444a7f2c5",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "\n",
    "-   `-b N, --batch-size N`: Set the batch size for prompt processing (default: `2048`). This large batch size benefits users who have BLAS installed and enabled it during the build. If you don't have BLAS enabled (\"BLAS=0\"), you can use a smaller number, such as 8, to see the prompt progress as it's evaluated in some situations.\n",
    "\n",
    "- `-ub N`, `--ubatch-size N`: physical maximum batch size. This is for pipeline parallelization. Default: `512`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9386e12-59d5-41f2-b8c2-6f5152e7d9d4",
   "metadata": {},
   "source": [
    "### Prompt Caching\n",
    "\n",
    "-   `--prompt-cache FNAME`: Specify a file to cache the model state after the initial prompt. This can significantly speed up the startup time when you're using longer prompts. The file is created during the first run and is reused and updated in subsequent runs. **Note**: Restoring a cached prompt does not imply restoring the exact state of the session at the point it was saved. So even when specifying a specific seed, you are not guaranteed to get the same sequence of tokens as the original generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b38173-24ac-4d49-94cf-5e54e2cf7724",
   "metadata": {},
   "source": [
    "### Grammars & JSON schemas\n",
    "\n",
    "-   `--grammar GRAMMAR`, `--grammar-file FILE`: Specify a grammar (defined inline or in a file) to constrain model output to a specific format. For example, you could force the model to output JSON or to speak only in emojis. See the [GBNF guide](../../grammars/README.md) for details on the syntax.\n",
    "\n",
    "-   `--json-schema SCHEMA`: Specify a [JSON schema](https://json-schema.org/) to constrain model output to (e.g. `{}` for any JSON object, or `{\"items\": {\"type\": \"string\", \"minLength\": 10, \"maxLength\": 100}, \"minItems\": 10}` for a JSON array of strings with size constraints). If a schema uses external `$ref`s, you should use `--grammar \"$( python examples/json_schema_to_grammar.py myschema.json )\"` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f1f2e-870f-418a-871d-f5d4527ffe9e",
   "metadata": {},
   "source": [
    "## 6. Additional Options\n",
    "\n",
    "These options provide extra functionality and customization when running the LLaMA models:\n",
    "\n",
    "-   `-h, --help`: Display a help message showing all available options and their default values. This is particularly useful for checking the latest options and default values, as they can change frequently, and the information in this document may become outdated.\n",
    "-   `--verbose-prompt`: Print the prompt before generating text.\n",
    "-   `-mg i, --main-gpu i`: When using multiple GPUs this option controls which GPU is used for small tensors for which the overhead of splitting the computation across all GPUs is not worthwhile. The GPU in question will use slightly more VRAM to store a scratch buffer for temporary results. By default GPU 0 is used.\n",
    "-   `-ts SPLIT, --tensor-split SPLIT`: When using multiple GPUs this option controls how large tensors should be split across all GPUs. `SPLIT` is a comma-separated list of non-negative values that assigns the proportion of data that each GPU should get in order. For example, \"3,2\" will assign 60% of the data to GPU 0 and 40% to GPU 1. By default the data is split in proportion to VRAM but this may not be optimal for performance.\n",
    "-   `--lora FNAME`: Apply a LoRA (Low-Rank Adaptation) adapter to the model (implies --no-mmap). This allows you to adapt the pretrained model to specific tasks or domains.\n",
    "-   `--lora-base FNAME`: Optional model to use as a base for the layers modified by the LoRA adapter. This flag is used in conjunction with the `--lora` flag, and specifies the base model for the adaptation.\n",
    "-   `-hfr URL --hf-repo URL`: The url to the Hugging Face model repository. Used in conjunction with `--hf-file` or `-hff`. The model is downloaded and stored in the file provided by `-m` or `--model`. If `-m` is not provided, the model is auto-stored in the path specified by the `LLAMA_CACHE` environment variable  or in an OS-specific local cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b88e2e-57f8-4d5b-b752-8671cabed28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
