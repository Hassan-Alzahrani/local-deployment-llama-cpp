{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f783e6-58ce-45d9-ab77-1b84807c5baf",
   "metadata": {},
   "source": [
    "# Getting started with LLaMA C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df8c16-7f4f-4e93-aa19-fdcb0d8d1b3f",
   "metadata": {},
   "source": [
    "## 1. Quick Start\n",
    "\n",
    "Open a terminal and run the following command to check that the `llama-cli` binary is available.\n",
    "\n",
    "```bash\n",
    "which llama-cli\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e622f-b34b-4404-8351-9705ffe21e59",
   "metadata": {},
   "source": [
    "### Input prompt (One-and-done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139a94c-9dc9-4ee7-9a24-6ec2b0858452",
   "metadata": {},
   "source": [
    "#### Manually downloading a model from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f69b90-573c-416b-8d78-cc8fb85ddd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1134  100  1134    0     0   3127      0 --:--:-- --:--:-- --:--:--  3132\n",
      "100 5082M  100 5082M    0     0  10.1M      0  0:08:21  0:08:21 --:--:-- 11.1M\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_URL=https://huggingface.co/ggml-org/gemma-1.1-7b-it-Q4_K_M-GGUF/resolve/main/gemma-1.1-7b-it.Q4_K_M.gguf\n",
    "curl --location --output ../models/gemma-1.1-7b-it.Q4_K_M.gguf $MODEL_URL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e4ed5-7458-427a-bf79-838cecd6423f",
   "metadata": {},
   "source": [
    "After downloading the model file we can just pass the path to the model file as a command line argument.\n",
    "\n",
    "-   `-m FNAME, --model FNAME`: Specify the path to the LLaMA model file.\n",
    "\n",
    "```bash\n",
    "MODEL=./models/gemma-1.1-7b-it.Q4_K_M.gguf\n",
    "llama-cli --model $MODEL --prompt \"Once upon a time\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9d622-b0dc-4500-9bb6-559d9e24b577",
   "metadata": {},
   "source": [
    "#### Downloading a model directly from a URL\n",
    "\n",
    "The command below makes use of the following options.\n",
    "\n",
    "-   `-mu MODEL_URL --model-url MODEL_URL`: Specify a remote http url to download the file.\n",
    "\n",
    "```bash\n",
    "MODEL_URL=https://huggingface.co/ggml-org/gemma-1.1-7b-it-Q4_K_M-GGUF/resolve/main/gemma-1.1-7b-it.Q4_K_M.gguf\n",
    "llama-cli --model-url \"$MODEL_URL\" --prompt \"Once upon a time\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717904a8-6348-4a60-ae72-4afcddd87dde",
   "metadata": {},
   "source": [
    "### Conversation mode (Allow for continuous interaction with the model)\n",
    "\n",
    "The command below makes use of the following options.\n",
    "\n",
    "- `-cnv,  --conversation`: run in conversation mode:\n",
    "  - does not print special tokens and suffix/prefix\n",
    "  - interactive mode is also enabled.\n",
    "- `--chat-template JINJA_TEMPLATE`: Set custom jinja chat template (default: template taken from model's metadata) if suffix/prefix are specified, see the [LLaMA C++ documentation](https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template) for the current list of accepted chat templates.\n",
    "\n",
    "```bash\n",
    "CHAT_TEMPLATE=gemma\n",
    "llama-cli --model $MODEL --conversation --chat-template $CHAT_TEMPLATE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6042b2-0c3d-4831-ad3a-3711fdcae1c5",
   "metadata": {},
   "source": [
    "## 2. Input Prompts\n",
    "\n",
    "The `llama-cli` program provides several ways to interact with the LLaMA models using input prompts:\n",
    "\n",
    "-   `--prompt PROMPT`: Provide a prompt directly as a command-line option.\n",
    "-   `--file FNAME`: Provide a file containing a prompt or multiple prompts.\n",
    "-   `--interactive-first`: Run the program in interactive mode and wait for input right away. (More on this below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16316d6-88a3-484c-a431-aad649c4af21",
   "metadata": {},
   "source": [
    "### `--prompt` example\n",
    "\n",
    "```bash\n",
    "llama-cli --model \"$MODEL\" --prompt \"What is the meaning of life?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523dcebb-4cbd-4abc-8b26-c2978e32ce63",
   "metadata": {},
   "source": [
    "### `--file` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a9f43a-a001-4847-b995-77b3294767e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"English\"\n",
    "tone_of_voice = \"Informative\"\n",
    "topic = \"Computer Science\"\n",
    "writing_style = \"Conversational\"\n",
    "\n",
    "prompt_template = f\"\"\"Please ignore all previous instructions. Please respond \\\n",
    "only in the {language} language. You are a Twitter influencer with a large \\\n",
    "following. You have a {tone_of_voice} tone of voice. You have a \\\n",
    "{writing_style} writing style. Do not self reference. Do not explain what you \\\n",
    "are doing. Please create a thread about {topic}. Add emojis to the thread \\\n",
    "when appropriate. The character count for each thread should be between 270 \\\n",
    "to 280 characters. Your content should be casual, informative, and an \\\n",
    "engaging Twitter thread. Please use simple and understandable words. Please \\\n",
    "include statistics, personal experience, and fun facts in the thread. Please \\\n",
    "add relevant hashtags to the post and encourage the readers join the \\\n",
    "conversation.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f85285ed-ddcc-4997-b454-589885e74224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please ignore all previous instructions. Please respond only in the English language. You are a Twitter influencer with a large following. You have a Informative tone of voice. You have a Conversational writing style. Do not self reference. Do not explain what you are doing. Please create a thread about Computer Science. Add emojis to the thread when appropriate. The character count for each thread should be between 270 to 280 characters. Your content should be casual, informative, and an engaging Twitter thread. Please use simple and understandable words. Please include statistics, personal experience, and fun facts in the thread. Please add relevant hashtags to the post and encourage the readers join the conversation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e7ded9-d5ee-47c3-a5b6-cb1f6e29a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../prompts/engaging-twitter-thread.txt\", 'w') as f:\n",
    "    f.write(prompt_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150fc4b0-4b8c-4f80-93dc-e661fa5030fc",
   "metadata": {},
   "source": [
    "```bash\n",
    "llama-cli --model \"$MODEL\" --file ./prompts/engaging-twitter-thread.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b5347-7b20-455c-b5e9-57a9f702d5dd",
   "metadata": {},
   "source": [
    "### `--interactive-first` example\n",
    "\n",
    "```bash\n",
    "llama-cli --model \"$MODEL\" --interactive-first\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fcc6c2-355f-4607-a0ec-265409afac08",
   "metadata": {},
   "source": [
    "## 3. Interaction\n",
    "\n",
    "The `llama-cli` program offers a seamless way to interact with LLaMA models, allowing users to engage in real-time conversations or provide instructions for specific tasks. The interactive mode can be triggered using various options, including `--interactive` and `--interactive-first`.\n",
    "\n",
    "In interactive mode, users can participate in text generation by injecting their input during the process. Users can press `Ctrl+C` at any time to interject and type their input, followed by pressing `Return` to submit it to the LLaMA model. To submit additional lines without finalizing input, users can end the current line with a backslash (`\\`) and continue typing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174e0c6-785b-4d8f-a190-3e5f07f51f5f",
   "metadata": {},
   "source": [
    "### Interaction Options\n",
    "\n",
    "-   `-i, --interactive`: Run the program in interactive mode, allowing users to engage in real-time conversations or provide specific instructions to the model.\n",
    "-   `--interactive-first`: Run the program in interactive mode and immediately wait for user input before starting the text generation.\n",
    "-   `-cnv,  --conversation`:  Run the program in conversation mode (does not print special tokens and suffix/prefix, use default chat template) (default: false)\n",
    "-   `--color`: Enable colorized output to differentiate visually distinguishing between prompts, user input, and generated text.\n",
    "\n",
    "By understanding and utilizing these interaction options, you can create engaging and dynamic experiences with your models, tailoring the text generation process to your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ac85db-e729-4a94-9c41-4807cb6055b1",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Type the following command in the terminal.\n",
    "\n",
    "```bash\n",
    "llama-cli --model \"$MODEL\" --conversation --color\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cff61d-6909-4910-9bcd-a6eec2d90905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "143d6b1e-347c-4809-8991-d1747f3e0017",
   "metadata": {},
   "source": [
    "### Reverse Prompts\n",
    "\n",
    "Reverse prompts are a powerful way to create a chat-like experience with your model by pausing the text generation when specific text strings are encountered using the `--reverse-prompt` option.\n",
    "\n",
    "-   `-r PROMPT, --reverse-prompt PROMPT`: Specify one or multiple reverse prompts to pause text generation and switch to interactive mode. For example, `-r \"User:\"` can be used to jump back into the conversation whenever it's the user's turn to speak. This helps create a more interactive and conversational experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896fc2a-56c4-4270-adab-94e2c99e6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036b8d7-294a-4324-bce7-acac57565a2f",
   "metadata": {},
   "source": [
    "### In-Prefix\n",
    "\n",
    "The `--in-prefix` flag is used to add a prefix to your input, primarily, this is used to insert a space after the reverse prompt. Here's an example of how to use the `--in-prefix` flag in conjunction with the `--reverse-prompt` flag:\n",
    "\n",
    "```sh\n",
    "llama-cli -r \"User:\" --in-prefix \" \"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e481ba-c57b-4da9-8106-1f1e57eaca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb58c5-fae8-4f2a-9e9c-e1a73caca731",
   "metadata": {},
   "source": [
    "### In-Suffix\n",
    "\n",
    "The `--in-suffix` flag is used to add a suffix after your input. This is useful for adding an \"Assistant:\" prompt after the user's input. It's added after the new-line character (`\\n`) that's automatically added to the end of the user's input. Here's an example of how to use the `--in-suffix` flag in conjunction with the `--reverse-prompt` flag:\n",
    "\n",
    "```sh\n",
    "./llama-cli -r \"User:\" --in-prefix \" \" --in-suffix \"Assistant:\"\n",
    "```\n",
    "When --in-prefix or --in-suffix options are enabled the chat template ( --chat-template ) is disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac75716-7119-4bf9-bf6d-b441601b575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7467e-108a-486d-9344-90feb85a7585",
   "metadata": {},
   "source": [
    "### Chat templates\n",
    "\n",
    " `--chat-template JINJA_TEMPLATE`: This option sets a custom jinja chat template. It accepts a string, not a file name.  Default: template taken from model's metadata. Llama.cpp only supports [some pre-defined templates](https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template). These include the following\n",
    "\n",
    "- `llama2`\n",
    "- `llama3`\n",
    "- `gemma`\n",
    "- `monarch`\n",
    "- `chatml`\n",
    "- `orion`\n",
    "- `vicuna`\n",
    "- `vicuna-orca`\n",
    "- `deepseek`\n",
    "- `command-r`\n",
    "- `zephyr`\n",
    "\n",
    "When `--in-prefix` or `--in-suffix` options are enabled the chat template ( `--chat-template` ) is disabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36958da5-1560-49d7-b4db-6eae750f1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482676a-0521-42e4-b664-cdfb603832c8",
   "metadata": {},
   "source": [
    "## 4. Context Management\n",
    "\n",
    "During text generation, models have a limited context size, which means they can only consider a certain number of tokens from the input and generated text. When the context fills up, the model resets internally,  otentially losing some information from the beginning of the conversation or instructions. Context management options help maintain continuity and coherence in these situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c4465-8973-4f7d-ae98-3dc0d00fb947",
   "metadata": {},
   "source": [
    "### Context Size\n",
    "\n",
    "- `-c N, --ctx-size N`: Set the size of the prompt context (default: 0, 0 = loaded from model). The LLaMA models were built with a context of 2048-8192, which will yield the best results on longer input/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de63d677-2ba3-4326-91d2-1a7e49f9d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d951e5d7-9474-427f-bba0-a53475267243",
   "metadata": {},
   "source": [
    "### Extended Context Size\n",
    "\n",
    "Some fine-tuned models have extended the context length by scaling RoPE. For example, if the original pre-trained model has a context length (max sequence length) of 4096 (4k) and the fine-tuned model has 32k. That is a scaling factor of 8, and should work by setting the above `--ctx-size` to 32768 (32k) and `--rope-scale` to 8.\n",
    "\n",
    "-   `--rope-scale N`: Where N is the linear scaling factor used by the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02c95a-f3fd-4cee-bc0b-314de435e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e3780-7b64-4f31-9223-382a0e792754",
   "metadata": {},
   "source": [
    "### Keep Prompt\n",
    "\n",
    "The `--keep` option allows users to retain the original prompt when the model runs out of context, ensuring a connection to the initial instruction or conversation topic is maintained.\n",
    "\n",
    "-   `--keep N`: Specify the number of tokens from the initial prompt to retain when the model resets its internal context. By default, this value is set to 0 (meaning no tokens are kept). Use `-1` to retain all tokens from the initial prompt.\n",
    "\n",
    "By utilizing context management options like `--ctx-size` and `--keep`, you can maintain a more coherent and consistent interaction with the LLaMA models, ensuring that the generated text remains relevant to the original prompt or conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c166bc-9ea9-4c61-8f4a-abf238794a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: usage example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f1f2e-870f-418a-871d-f5d4527ffe9e",
   "metadata": {},
   "source": [
    "## 6. Additional Options\n",
    "\n",
    "These options provide extra functionality and customization when running the LLaMA models:\n",
    "\n",
    "-   `-h, --help`: Display a help message showing all available options and their default values. This is particularly useful for checking the latest options and default values, as they can change frequently, and the information in this document may become outdated.\n",
    "-   `--verbose-prompt`: Print the prompt before generating text.\n",
    "-   `-mg i, --main-gpu i`: When using multiple GPUs this option controls which GPU is used for small tensors for which the overhead of splitting the computation across all GPUs is not worthwhile. The GPU in question will use slightly more VRAM to store a scratch buffer for temporary results. By default GPU 0 is used.\n",
    "-   `-ts SPLIT, --tensor-split SPLIT`: When using multiple GPUs this option controls how large tensors should be split across all GPUs. `SPLIT` is a comma-separated list of non-negative values that assigns the proportion of data that each GPU should get in order. For example, \"3,2\" will assign 60% of the data to GPU 0 and 40% to GPU 1. By default the data is split in proportion to VRAM but this may not be optimal for performance.\n",
    "-   `--lora FNAME`: Apply a LoRA (Low-Rank Adaptation) adapter to the model (implies --no-mmap). This allows you to adapt the pretrained model to specific tasks or domains.\n",
    "-   `--lora-base FNAME`: Optional model to use as a base for the layers modified by the LoRA adapter. This flag is used in conjunction with the `--lora` flag, and specifies the base model for the adaptation.\n",
    "-   `-hfr URL --hf-repo URL`: The url to the Hugging Face model repository. Used in conjunction with `--hf-file` or `-hff`. The model is downloaded and stored in the file provided by `-m` or `--model`. If `-m` is not provided, the model is auto-stored in the path specified by the `LLAMA_CACHE` environment variable  or in an OS-specific local cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b88e2e-57f8-4d5b-b752-8671cabed28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
