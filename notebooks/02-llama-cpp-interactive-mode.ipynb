{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e8fe35-2dd7-48ea-9c34-34259e584446",
   "metadata": {},
   "source": [
    "### Interactive mode\n",
    "\n",
    "> [!NOTE]\n",
    "> If you prefer basic usage, please consider using conversation mode instead of interactive mode\n",
    "\n",
    "In this mode, you can always interrupt generation by pressing Ctrl+C and entering one or more lines of text, which will be converted into tokens and appended to the current context. You can also specify a *reverse prompt* with the parameter `-r \"reverse prompt string\"`. This will result in user input being prompted whenever the exact tokens of the reverse prompt string are encountered in the generation. A typical use is to use a prompt that makes LLaMA emulate a chat between multiple users, say Alice and Bob, and pass `-r \"Alice:\"`.\n",
    "\n",
    "Here is an example of a few-shot interaction, invoked with the command\n",
    "\n",
    "```bash\n",
    "# default arguments using a 7B model\n",
    "./examples/chat.sh\n",
    "\n",
    "# advanced chat with a 13B model\n",
    "./examples/chat-13B.sh\n",
    "\n",
    "# custom arguments using a 13B model\n",
    "./llama-cli -m ./models/13B/ggml-model-q4_0.gguf -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt\n",
    "```\n",
    "\n",
    "Note the use of `--color` to distinguish between user input and generated text. Other parameters are explained in more detail in the [README](examples/main/README.md) for the `llama-cli` example program.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/1991296/224575029-2af3c7dc-5a65-4f64-a6bb-517a532aea38.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eba8df-180f-4380-8acf-9c76680afdbe",
   "metadata": {},
   "source": [
    "### Persistent Interaction\n",
    "\n",
    "The prompt, user inputs, and model generations can be saved and resumed across calls to `./llama-cli` by leveraging `--prompt-cache` and `--prompt-cache-all`. The `./examples/chat-persistent.sh` script demonstrates this with support for long-running, resumable chat sessions. To use this example, you must provide a file to cache the initial chat prompt and a directory to save the chat session, and may optionally provide the same variables as `chat-13B.sh`. The same prompt cache can be reused for new chat sessions. Note that both prompt cache and chat directory are tied to the initial prompt (`PROMPT_TEMPLATE`) and the model file.\n",
    "\n",
    "```bash\n",
    "# Start a new chat\n",
    "PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/default ./examples/chat-persistent.sh\n",
    "\n",
    "# Resume that chat\n",
    "PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/default ./examples/chat-persistent.sh\n",
    "\n",
    "# Start a different chat with the same prompt/model\n",
    "PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/another ./examples/chat-persistent.sh\n",
    "\n",
    "# Different prompt cache for different prompt/model\n",
    "PROMPT_TEMPLATE=./prompts/chat-with-bob.txt PROMPT_CACHE_FILE=bob.prompt.bin \\\n",
    "    CHAT_SAVE_DIR=./chat/bob ./examples/chat-persistent.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
