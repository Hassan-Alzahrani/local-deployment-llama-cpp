gguf
llama-cpp-python --config-settings cmake.args="-DGGML_CUDA=ON;-DGGML_LLAMAFILE=OFF;-DGGML_BLAS=ON;-DGGML_BLAS_VENDOR=OpenBLAS"
