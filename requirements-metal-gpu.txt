gguf
llama-cpp-python --config-settings cmake.args="-DLLAMA_CURL=ON;-DGGML_ACCELERATE=ON;-DGGML_METAL=ON;-DGGML_LLAMAFILE=OFF"
